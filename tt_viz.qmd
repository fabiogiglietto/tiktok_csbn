---
title: "TikTok Coordinated Detection Project"
author: "Fabio Giglietto"
date: "`r Sys.Date()`"
render_with_liquid: false
format: 
  html:
    code-fold: true
    toc: true
    number-sections: true
    include-in-header:
      - text: |
          <style>
          /* Vera.ai Brand Styles */
          @import url('https://fonts.googleapis.com/css2?family=Comfortaa:wght@300;700&display=swap');
          
          :root {
            --vera-green: #00926c;
            --vera-dark: #3d3d3c;
            --vera-white: #ffffff;
          }
          
          body {
            font-family: 'Comfortaa', sans-serif;
            color: var(--vera-dark);
          }
          
          /* Table Styles */
          .dataTables_wrapper {
            font-family: 'Comfortaa', sans-serif !important;
          }
          
          .dataTables_wrapper .dataTables_filter input {
            border: 1px solid var(--vera-green);
            border-radius: 4px;
            padding: 4px 8px;
          }
          
          .dataTables_wrapper .dataTables_length select {
            border: 1px solid var(--vera-green);
            border-radius: 4px;
          }
          
          .dataTables_wrapper .dataTables_paginate .paginate_button.current {
            background: var(--vera-green) !important;
            color: var(--vera-white) !important;
            border: none;
            border-radius: 4px;
          }
          
          .dataTables_wrapper .dataTables_paginate .paginate_button:hover {
            background: var(--vera-dark) !important;
            color: var(--vera-white) !important;
            border: none;
          }
          
          /* Chart Styles */
          .recharts-default-tooltip {
            background-color: var(--vera-white) !important;
            border: 1px solid var(--vera-green) !important;
            border-radius: 4px;
            font-family: 'Comfortaa', sans-serif;
          }
          
          /* Network Tooltip Styles */
          .network-tooltip {
            font-family: 'Comfortaa', sans-serif;
            background-color: var(--vera-white);
            border: 1px solid var(--vera-green);
            border-radius: 4px;
            padding: 12px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
          }
          
          .network-tooltip a {
            color: var(--vera-green);
            text-decoration: none;
          }
          
          .network-tooltip a:hover {
            text-decoration: underline;
          }
          
          /* Network Visualization Styles */
          .vis-network {
            background-color: var(--vera-white);
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
          }
          
          .vis-network .vis-navigation {
            font-family: 'Comfortaa', sans-serif;
          }
          
          /* Layout Fixes */
          #quarto-content {
            grid-template-rows: none !important;
            grid-auto-rows: min-content !important;
            font-family: 'Comfortaa', sans-serif;
          }
          
          /* Table Cell Wrapping */
          .dt-head-left, 
          .dt-body-left {
            white-space: normal !important;
            color: var(--vera-dark);
            padding: 8px !important;
          }
          
          /* DataTables Cell Links */
          .dt-head-left a, 
          .dt-body-left a {
            color: var(--vera-green);
            text-decoration: none;
          }
          
          .dt-head-left a:hover, 
          .dt-body-left a:hover {
            text-decoration: underline;
          }
          
          /* Card Styles */
          .card {
            background: var(--vera-white);
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            padding: 24px;
            margin-bottom: 24px;
          }
          
          .card-title {
            color: var(--vera-dark);
            font-family: 'Comfortaa', sans-serif;
            font-weight: 700;
            font-size: 1.5rem;
            margin-bottom: 16px;
          }
          
          /* Button Styles */
          .vera-button {
            background-color: var(--vera-green);
            color: var(--vera-white);
            border: none;
            border-radius: 4px;
            padding: 8px 16px;
            font-family: 'Comfortaa', sans-serif;
            cursor: pointer;
            transition: background-color 0.2s;
          }
          
          .vera-button:hover {
            background-color: var(--vera-dark);
          }
          </style>  
params:
  load_from_snapshot: FALSE
  base_dir: "/home/mine/VERAAI_WP4"
  time_window: 180
  min_participation: 2
  edge_weight: 0.5
  days_back: 15
  ai_model: "chatgpt-4o-latest"
  openai_key: "default"
  openai_org: "default"
---

# Introduction

This document serves as a proof of concept, developed within the framework of the [vera.ai Horizon EU project](https://www.veraai.eu/). It presents a comprehensive methodology for tracking and analyzing coordinated sharing activities on TikTok, employing [`traktok`](https://github.com/JBGruber/traktok) for data collection via the TikTok Research API and [`CooRTweet`](https://github.com/nicolarighetti/CooRTweet) for the analysis of coordinated behavior patterns.

**Initial Discovery:** The analysis initiates by focusing on content tagged with the #moskow hashtag. This first step successfully identifies a preliminary group of accounts involved in coordinated sharing activities in the aftermath of the attack in Moscow. This discovery lays the groundwork for an extensive examination of coordinated dynamics across social media platforms.

On April 19, 2024, we received a list of 513 problematic accounts from a trusted partner within the <i>vera ai</i> consortium. Accounts that were mentioned at least twice on this list have been added to our pool of monitored accounts.

```{r write_description, results='asis', echo=FALSE}
cat(sprintf(
"**Daily Monitoring and Analysis:** Subsequent to the initial identification, the methodology transitions into a phase of daily monitoring. In this phase, the script consistently retrieves videos posted by the previously identified accounts, with the goal of detecting both ongoing and emerging instances of coordinated behavior. As new accounts manifesting coordinated behavior (time_window = %d seconds, min_participation = %d posts, edge_weight = %.1f) are discovered, they are incorporated into the daily monitoring routine. For each identified network of coordinated accounts, AI-generated labels are automatically created using the %s model by analyzing the content patterns, themes, and characteristics of the posts shared within each network, providing a concise description of the common elements that unite these accounts in their coordinated behavior.", 
params$time_window,
params$min_participation,
params$edge_weight,
params$ai_model
))

```

This approach ensures continuous updates on the number of newly discovered coordinated accounts, highlighting the fluid nature of social media coordination. Enhanced by interactive visualizations, the analysis sheds light on the shifting landscape of account activities and the intricate network of interactions among them on the TikTok platform.

By delineating these processes, the proof of concept underscores the potential for advanced analytical tools to reveal and understand the complex phenomena of coordinated social media behavior within the context of significant societal events.

```{r setup, include=FALSE, message=FALSE}
# Set working directory
setwd(params$base_dir)

# Library loading
library(tidyverse)
library(CooRTweet)
library(httr)
library(purrr)
library(scales)
library(stringr)
library(igraph)
library(visNetwork)
library(ggplot2)
library(jsonlite)
library(lubridate)
library(progress)
library(logger)
library(traktok)
library(DT)
library(htmltools)

# Initialize logging
log_info("Starting TikTok coordinated activity detection script")

# Initialize global error list
global_error_list <- list()

# Quota monitoring
quota_status <- list(
  daily_limit_reached = FALSE,
  rate_limit_active = FALSE,
  last_quota_error = NULL
)

# Function to check if we should continue processing
should_continue_processing <- function() {
  if (quota_status$daily_limit_reached) {
    log_warn("Daily quota limit reached - skipping further API calls")
    return(FALSE)
  }
  return(TRUE)
}

# Function to update quota status based on error
update_quota_status <- function(error_msg) {
  if (grepl("daily_quota_limit_exceeded", error_msg)) {
    quota_status$daily_limit_reached <<- TRUE
    quota_status$last_quota_error <<- Sys.time()
    log_error("Daily quota limit exceeded - no further API calls will be made")
  } else if (grepl("rate_limit_exceeded", error_msg)) {
    quota_status$rate_limit_active <<- TRUE
    log_warn("Rate limit active - will retry with backoff")
  }
}

# Enhanced API call function with rate limiting and retry logic
safe_api_call <- function(api_function, max_retries = 3, base_delay = 2) {
  for (attempt in 1:max_retries) {
    tryCatch({
      result <- api_function()
      return(result)
    }, error = function(e) {
      error_msg <- conditionMessage(e)
      update_quota_status(error_msg)
      
      if (grepl("429", error_msg)) {
        if (grepl("daily_quota_limit_exceeded", error_msg)) {
          log_error("Daily quota limit exceeded - cannot retry until quota resets")
          global_error_list <<- c(global_error_list, list(list(
            type = "daily_quota_exceeded",
            message = error_msg,
            timestamp = Sys.time()
          )))
          stop("Daily quota exceeded")
        } else if (grepl("rate_limit_exceeded", error_msg)) {
          if (attempt < max_retries) {
            delay <- base_delay * (2 ^ (attempt - 1))  # Exponential backoff
            log_warn(sprintf("Rate limit exceeded, attempt %d/%d. Waiting %d seconds...", 
                           attempt, max_retries, delay))
            Sys.sleep(delay)
          } else {
            log_error("Rate limit exceeded - max retries reached")
            stop(error_msg)
          }
        }
      } else {
        # For non-rate-limit errors, don't retry
        stop(error_msg)
      }
    })
  }
  stop("Max retries exceeded")
}

# Secure credential retrieval
get_tiktok_credentials <- function() {
  list(
    client_key = Sys.getenv("TIKTOK_CLIENT_KEY"),
    client_secret = Sys.getenv("TIKTOK_CLIENT_SECRET")
  )
}

# Authentication using traktok package
request_access_token <- function() {
  tryCatch({
    token <- traktok::auth_research(
      client_key = Sys.getenv("TIKTOK_CLIENT_KEY"),
      client_secret = Sys.getenv("TIKTOK_CLIENT_SECRET")
    )
    log_info("Successfully obtained access token using traktok")
    return(token$access_token)
  }, error = function(e) {
    log_error(paste("Error in request_access_token using traktok:", e$message))
    stop(e)
  })
}

# Add new function to create daily directory
create_daily_output_dir <- function() {
  today_date <- format(Sys.Date(), "%Y-%m-%d")
  daily_dir <- file.path(params$base_dir, "output", "daily", today_date)
  dir.create(daily_dir, showWarnings = FALSE, recursive = TRUE)
  
  # Create subdirectories for different types of output
  dir.create(file.path(daily_dir, "coordination"), showWarnings = FALSE)
  dir.create(file.path(daily_dir, "network"), showWarnings = FALSE)
  dir.create(file.path(daily_dir, "summary"), showWarnings = FALSE)
  
  return(daily_dir)
}

# CORRECTED setup function without setwd
setup <- function() {
  credentials <- get_tiktok_credentials()
  
  if (!params$load_from_snapshot) {
    # Ensure environment variables are set
    if (is.na(Sys.getenv("TIKTOK_CLIENT_KEY")) | is.na(Sys.getenv("TIKTOK_CLIENT_SECRET"))) {
      log_error("Environment variables for TikTok API not set")
      stop("Environment variables for TikTok API not set. Please set TIKTOK_CLIENT_KEY and TIKTOK_CLIENT_SECRET.")
    }
    
    tryCatch({
      access_token <- request_access_token()
    }, error = function(e) {
      log_error(paste("Authentication with TikTok API failed:", e$message))
      stop("Authentication with TikTok API failed. Error message: ", e$message)
    })
    
  } else {
    # Use file.path instead of setwd for file operations
    snapshot_path <- file.path(params$base_dir, "output", "latest_snapshot.csv")
    all_videos <- readr::read_csv(snapshot_path,
                                 col_types = cols(video_id = col_character(), 
                                                effect_ids = col_character(),
                                                music_id = col_character(), 
                                                hashtag_names = col_character()))
    recent_videos <- NA
  }
  
  # Try reading the CSV file with full path
  tryCatch({
    accounts_file_path <- file.path(params$base_dir, "lists", "tiktok_coordinated_accounts_ids.csv")
    account_ids <- readr::read_csv(accounts_file_path)
  }, error = function(e) {
    log_error(paste("Failed to read TikTok coordinated accounts ID CSV:", e$message))
    stop("Failed to read TikTok coordinated accounts ID CSV. Error: ", e$message)
  })
  
  coord_users <- unique(account_ids$x)
  enddate <- Sys.Date()
  # Use the days_back parameter instead of hardcoded value
  startdate <- enddate - params$days_back
  
  # Create a list of values to return but don't include params
  setup_values <- list(
    coord_users = coord_users,
    enddate = enddate,
    startdate = startdate,
    access_token = if(exists("access_token")) access_token else NULL,
    all_videos = if(exists("all_videos")) all_videos else NULL
  )
  
  # Assign values to global environment individually
  for(name in names(setup_values)) {
    assign(name, setup_values[[name]], envir = .GlobalEnv)
  }
}

# Create daily directory
daily_output_dir <- create_daily_output_dir()

# Run setup without changing working directory
setup()
```

```{r smart_reuse_setup, include=FALSE, message=FALSE}

# Function to check if recent_videos_latest.csv is fresh (less than 23 hours old)
is_recent_videos_fresh <- function() {
  backup_path <- file.path(params$base_dir, "output", "recent_videos_latest.csv")
  
  if (!file.exists(backup_path)) {
    log_info("recent_videos_latest.csv not found - need fresh fetch")
    return(FALSE)
  }
  
  # Get file modification time
  file_info <- file.info(backup_path)
  file_mod_time <- file_info$mtime
  
  # Calculate hours since last modification
  hours_since_modified <- as.numeric(difftime(Sys.time(), file_mod_time, units = "hours"))
  
  log_info(sprintf("recent_videos_latest.csv is %.1f hours old", hours_since_modified))
  
  # Return TRUE if less than 23 hours old
  is_fresh <- hours_since_modified < 23
  
  if (is_fresh) {
    log_info("✅ recent_videos_latest.csv is FRESH (< 23 hours) - will use cached data")
  } else {
    log_info("⚠️ recent_videos_latest.csv is STALE (>= 23 hours) - need fresh fetch")
  }
  
  return(is_fresh)
}

# Function to load recent videos from cache
load_recent_videos_from_cache <- function() {
  backup_path <- file.path(params$base_dir, "output", "recent_videos_latest.csv")
  
  tryCatch({
    cached_videos <- readr::read_csv(backup_path,
                                   col_types = cols(
                                     video_id = col_character(),
                                     create_time = col_character(),
                                     fetch_timestamp = col_character(),
                                     fetch_date = col_character(),
                                     data_source = col_character(),
                                     effect_ids = col_character(),
                                     music_id = col_character(),
                                     hashtag_names = col_character(),
                                     author_name = col_character()
                                   ))
    
    log_info(sprintf("✅ Loaded %d videos from cache", nrow(cached_videos)))
    
    # Filter for current monitoring scope (coord_users and date range)
    if (exists("coord_users") && exists("startdate") && exists("enddate")) {
      filtered_videos <- cached_videos %>%
        filter(
          author_name %in% coord_users,
          as.Date(create_time) >= as.Date(startdate),
          as.Date(create_time) <= as.Date(enddate)
        )
      
      log_info(sprintf("📋 Filtered to %d videos for current monitoring scope", nrow(filtered_videos)))
      
      # Remove metadata columns for consistency with fresh fetches
      filtered_videos <- filtered_videos %>%
        select(-any_of(c("fetch_timestamp", "fetch_date", "data_source")))
      
      return(filtered_videos)
    }
    
    return(cached_videos)
    
  }, error = function(e) {
    log_error(sprintf("Failed to load cache: %s", e$message))
    return(NULL)
  })
}

# Enhanced main fetch function with 23-hour cache check
fetch_recent_videos_smart <- function() {
  if (!params$load_from_snapshot) {
    log_info("=== STARTING SMART VIDEO COLLECTION WITH 23-HOUR CACHE CHECK ===")
    
    # FIRST: Check if we can use the cached data
    if (is_recent_videos_fresh()) {
      log_info("🎯 Using cached recent videos (less than 23 hours old)")
      
      cached_videos <- load_recent_videos_from_cache()
      
      if (!is.null(cached_videos) && nrow(cached_videos) > 0) {
        log_info(sprintf("✅ Successfully loaded %d videos from cache", nrow(cached_videos)))
        
        # Update daily snapshot with cached data
        recent_videos_path <- file.path(daily_output_dir, "recent_videos_snapshot.csv")
        tryCatch({
          # Add back metadata for daily snapshot
          cached_with_metadata <- cached_videos %>%
            mutate(
              fetch_timestamp = as.character(Sys.time()),
              fetch_date = as.character(Sys.Date()),
              data_source = "loaded_from_23h_cache"
            )
          
          readr::write_csv(cached_with_metadata, recent_videos_path)
          log_info(sprintf("✅ Copied cached data to daily snapshot: %s", recent_videos_path))
        }, error = function(e) {
          log_warn(sprintf("Failed to update daily snapshot: %s", e$message))
        })
        
        return(cached_videos)
      } else {
        log_warn("Cache load failed, proceeding with fresh fetch")
      }
    }
    
    # SECOND: If cache is stale or doesn't exist, proceed with smart reuse logic
    log_info("📊 Cache is stale or unavailable - proceeding with smart fetch")
    
    # Check if we have smart reuse results from yesterday
    if (!exists("smart_reuse_result")) {
      log_warn("Smart reuse not available, falling back to normal fetch")
      return(fetch_recent_videos())
    }
    
    # Initialize combined results with reusable videos
    combined_results <- smart_reuse_result$reusable_videos
    
    # If we have accounts that need fresh fetching
    if (length(smart_reuse_result$accounts_to_fetch) > 0) {
      log_info(sprintf("Fetching fresh data for %d accounts", 
                      length(smart_reuse_result$accounts_to_fetch)))
      
      # Fetch fresh videos for accounts not covered by yesterday's data
      fresh_videos_result <- get_recent_videos(
        usernames = smart_reuse_result$accounts_to_fetch,
        start_date = smart_reuse_result$fetch_date_range$start,
        end_date = smart_reuse_result$fetch_date_range$end,
        chunk_size = 20,
        max_pages = 3
      )
      
      fresh_videos <- fresh_videos_result$data
      
      # Ensure data type consistency before combining
      if (nrow(fresh_videos) > 0) {
        # Standardize both datasets to ensure compatibility
        combined_results <- standardize_data_types(combined_results, "reused")
        fresh_videos <- standardize_data_types(fresh_videos, "fresh")
        
        combined_results <- bind_rows(combined_results, fresh_videos)
        log_info(sprintf("Combined %d reused + %d fresh = %d total recent videos", 
                        nrow(smart_reuse_result$reusable_videos),
                        nrow(fresh_videos),
                        nrow(combined_results)))
      } else {
        log_info(sprintf("No fresh videos found, using %d reused videos", 
                        nrow(combined_results)))
      }
    } else {
      log_info(sprintf("Using only %d reused videos, no fresh fetch needed", 
                      nrow(combined_results)))
    }
    
    # Remove duplicates based on video_id
    if (nrow(combined_results) > 0) {
      initial_count <- nrow(combined_results)
      combined_results <- combined_results %>% 
        distinct(video_id, .keep_all = TRUE)
      final_count <- nrow(combined_results)
      
      if (initial_count != final_count) {
        log_info(sprintf("Removed %d duplicate videos", initial_count - final_count))
      }
      
      # Log final user count with proper column detection
      user_column <- if("author_name" %in% colnames(combined_results)) "author_name" else "username"
      log_info(sprintf("Final unique users: %d", length(unique(combined_results[[user_column]]))))
    }
    
    # === IMMEDIATE STORAGE OF RECENT VIDEOS ===
    if (nrow(combined_results) > 0) {
      # Create storage paths
      recent_videos_path <- file.path(daily_output_dir, "recent_videos_snapshot.csv")
      
      # Store with metadata about the fetch
      recent_videos_with_metadata <- combined_results %>%
        mutate(
          fetch_timestamp = as.character(Sys.time()),
          fetch_date = as.character(Sys.Date()),
          data_source = case_when(
            video_id %in% smart_reuse_result$reusable_videos$video_id ~ "reused_from_yesterday",
            TRUE ~ "fresh_api_fetch"
          )
        )
      
      # Save recent videos immediately
      tryCatch({
        readr::write_csv(recent_videos_with_metadata, recent_videos_path)
        log_info(sprintf("✅ STORED recent videos to: %s", recent_videos_path))
        log_info(sprintf("   - Total videos: %d", nrow(recent_videos_with_metadata)))
        log_info(sprintf("   - Reused: %d, Fresh: %d", 
                        sum(recent_videos_with_metadata$data_source == "reused_from_yesterday"),
                        sum(recent_videos_with_metadata$data_source == "fresh_api_fetch")))
      }, error = function(e) {
        log_error(sprintf("Failed to store recent videos: %s", e$message))
      })
      
      # CRITICAL: Update the recent_videos_latest.csv file (23-hour cache)
      backup_path <- file.path(params$base_dir, "output", "recent_videos_latest.csv")
      tryCatch({
        readr::write_csv(recent_videos_with_metadata, backup_path)
        log_info(sprintf("✅ UPDATED 23-hour cache: %s", backup_path))
        log_info("⏰ This cache will be used for the next 23 hours")
      }, error = function(e) {
        log_error(sprintf("❌ Failed to update 23-hour cache: %s", e$message))
      })
    } else {
      log_warn("No recent videos to store")
    }
    
    log_info(sprintf("=== SMART COLLECTION COMPLETED ==="))
    log_info(sprintf("Total videos: %d", nrow(combined_results)))
    log_info(sprintf("API calls saved: %d", smart_reuse_result$api_calls_saved))
    
    return(combined_results)
  } else {
    log_info("Loading from snapshot, skipping video collection")
    return(NULL)
  }
}

# Function to get yesterday's data file path
get_yesterday_data_path <- function() {
  yesterday_date <- format(Sys.Date() - 1, "%Y-%m-%d")
  yesterday_dir <- file.path(params$base_dir, "output", "daily", yesterday_date)
  yesterday_snapshot <- file.path(yesterday_dir, "daily_snapshot.csv")
  return(list(
    dir = yesterday_dir,
    snapshot = yesterday_snapshot,
    date = yesterday_date
  ))
}

# Function to standardize data types between yesterday's data and fresh API data
standardize_data_types <- function(df, source_name = "data") {
  if (nrow(df) == 0) return(df)
  
  df %>%
    mutate(
      # Ensure consistent character types for these fields
      create_time = as.character(create_time),
      video_id = as.character(video_id),
      
      # Convert list columns to character (API returns lists, CSV loads as character)
      effect_ids = if("effect_ids" %in% colnames(.)) {
        if(is.list(effect_ids)) sapply(effect_ids, function(x) paste(x, collapse = ",")) else as.character(effect_ids)
      } else NA_character_,
      
      music_id = if("music_id" %in% colnames(.)) {
        if(is.list(music_id)) sapply(music_id, function(x) paste(x, collapse = ",")) else as.character(music_id)
      } else NA_character_,
      
      hashtag_names = if("hashtag_names" %in% colnames(.)) {
        if(is.list(hashtag_names)) sapply(hashtag_names, function(x) paste(x, collapse = ",")) else as.character(hashtag_names)
      } else NA_character_,
      
      # Ensure numeric columns are numeric
      view_count = if("view_count" %in% colnames(.)) as.numeric(view_count) else NA_real_,
      like_count = if("like_count" %in% colnames(.)) as.numeric(like_count) else NA_real_,
      comment_count = if("comment_count" %in% colnames(.)) as.numeric(comment_count) else NA_real_,
      share_count = if("share_count" %in% colnames(.)) as.numeric(share_count) else NA_real_
    )
}
calculate_date_overlap <- function(start_date, end_date, yesterday_data) {
  # Convert dates to Date objects if they aren't already
  start_date <- as.Date(start_date)
  end_date <- as.Date(end_date)
  
  # Get the date range from yesterday's data
  yesterday_videos_dates <- as.Date(yesterday_data$create_time)
  yesterday_min_date <- min(yesterday_videos_dates, na.rm = TRUE)
  yesterday_max_date <- max(yesterday_videos_dates, na.rm = TRUE)
  
  # Calculate overlap period
  overlap_start <- max(start_date, yesterday_min_date)
  overlap_end <- min(end_date, yesterday_max_date)
  
  # Check if there's actual overlap
  has_overlap <- overlap_start <= overlap_end
  
  return(list(
    has_overlap = has_overlap,
    overlap_start = if(has_overlap) overlap_start else NULL,
    overlap_end = if(has_overlap) overlap_end else NULL,
    new_start = if(has_overlap) max(overlap_end + 1, start_date) else start_date,
    new_end = end_date
  ))
}

# Function to load and filter yesterday's data
load_yesterday_data <- function() {
  yesterday_paths <- get_yesterday_data_path()
  
  if (!file.exists(yesterday_paths$snapshot)) {
    log_info(sprintf("No yesterday data found at %s", yesterday_paths$snapshot))
    return(NULL)
  }
  
  tryCatch({
    yesterday_data <- readr::read_csv(yesterday_paths$snapshot, 
                                    col_types = cols(
                                      video_id = col_character(), 
                                      effect_ids = col_character(),
                                      music_id = col_character(), 
                                      hashtag_names = col_character(),
                                      create_time = col_character()
                                    ))
    
    log_info(sprintf("Successfully loaded %d videos from yesterday (%s)", 
                    nrow(yesterday_data), yesterday_paths$date))
    
    return(yesterday_data)
  }, error = function(e) {
    log_error(sprintf("Error loading yesterday's data: %s", e$message))
    return(NULL)
  })
}

# Function to filter yesterday's data for current date range
filter_reusable_videos <- function(yesterday_data, start_date, end_date) {
  if (is.null(yesterday_data) || nrow(yesterday_data) == 0) {
    return(list(
      reusable_videos = data.frame(),
      accounts_covered = character(),
      date_overlap = list(has_overlap = FALSE)
    ))
  }
  
  # Calculate date overlap
  date_overlap <- calculate_date_overlap(start_date, end_date, yesterday_data)
  
  if (!date_overlap$has_overlap) {
    log_info("No date overlap with yesterday's data")
    return(list(
      reusable_videos = data.frame(),
      accounts_covered = character(),
      date_overlap = date_overlap
    ))
  }
  
  # Filter videos within the overlap period
  reusable_videos <- yesterday_data %>%
    filter(
      as.Date(create_time) >= date_overlap$overlap_start,
      as.Date(create_time) <= date_overlap$overlap_end
    )
  
  # Standardize reusable_videos data types
  if (nrow(reusable_videos) > 0) {
    reusable_videos <- standardize_data_types(reusable_videos, "reusable")
  }
  
  # Get accounts that are covered by reusable videos
  accounts_covered <- unique(reusable_videos$author_name)
  
  log_info(sprintf("Found %d reusable videos covering %d accounts from overlap period %s to %s", 
                  nrow(reusable_videos), 
                  length(accounts_covered),
                  date_overlap$overlap_start,
                  date_overlap$overlap_end))
  
  return(list(
    reusable_videos = reusable_videos,
    accounts_covered = accounts_covered,
    date_overlap = date_overlap
  ))
}

# Function to determine which accounts need fresh API calls
get_accounts_needing_update <- function(coord_users, accounts_covered, date_overlap) {
  if (!date_overlap$has_overlap) {
    # No overlap, need to fetch all accounts
    return(list(
      accounts_to_fetch = coord_users,
      accounts_reused = character(),
      fetch_date_range = list(start = startdate, end = enddate)
    ))
  }
  
  # Accounts that are NOT covered by yesterday's data
  accounts_to_fetch <- setdiff(coord_users, accounts_covered)
  accounts_reused <- intersect(coord_users, accounts_covered)
  
  log_info(sprintf("Accounts analysis: %d total, %d reused, %d need fresh fetch", 
                  length(coord_users), 
                  length(accounts_reused), 
                  length(accounts_to_fetch)))
  
  return(list(
    accounts_to_fetch = accounts_to_fetch,
    accounts_reused = accounts_reused,
    fetch_date_range = list(
      start = date_overlap$new_start,
      end = date_overlap$new_end
    )
  ))
}

# Main smart reuse function
implement_smart_reuse <- function(coord_users, start_date, end_date) {
  log_info("=== IMPLEMENTING SMART VIDEO REUSE ===")
  
  # Step 1: Load yesterday's data
  yesterday_data <- load_yesterday_data()
  
  # Step 2: Filter reusable videos
  reuse_analysis <- filter_reusable_videos(yesterday_data, start_date, end_date)
  
  # Step 3: Determine what accounts need fresh API calls
  fetch_analysis <- get_accounts_needing_update(
    coord_users, 
    reuse_analysis$accounts_covered, 
    reuse_analysis$date_overlap
  )
  
  # Step 4: Log the strategy
  if (length(fetch_analysis$accounts_to_fetch) == 0) {
    log_info("🎉 ALL VIDEOS CAN BE REUSED! No API calls needed for recent videos.")
  } else if (length(fetch_analysis$accounts_reused) > 0) {
    log_info(sprintf("📊 HYBRID STRATEGY: Reusing %d accounts, fetching %d accounts", 
                    length(fetch_analysis$accounts_reused),
                    length(fetch_analysis$accounts_to_fetch)))
    log_info(sprintf("📅 Fetching date range: %s to %s", 
                    fetch_analysis$fetch_date_range$start,
                    fetch_analysis$fetch_date_range$end))
  } else {
    log_info("🔄 FULL FETCH: No reusable data, fetching all accounts")
  }
  
  return(list(
    reusable_videos = reuse_analysis$reusable_videos,
    accounts_to_fetch = fetch_analysis$accounts_to_fetch,
    accounts_reused = fetch_analysis$accounts_reused,
    fetch_date_range = fetch_analysis$fetch_date_range,
    api_calls_saved = length(fetch_analysis$accounts_reused)
  ))
}

# Execute smart reuse analysis
if (exists("coord_users") && exists("startdate") && exists("enddate")) {
  smart_reuse_result <- implement_smart_reuse(coord_users, startdate, enddate)
  
  # Store results globally for use in fetch functions
  assign("smart_reuse_result", smart_reuse_result, envir = .GlobalEnv)
  
  log_info(sprintf("=== SMART REUSE SUMMARY ==="))
  log_info(sprintf("API calls potentially saved: %d", smart_reuse_result$api_calls_saved))
  log_info(sprintf("Accounts to fetch: %d", length(smart_reuse_result$accounts_to_fetch)))
  log_info(sprintf("Reusable videos: %d", nrow(smart_reuse_result$reusable_videos)))
} else {
  log_warn("Missing required variables for smart reuse analysis")
  smart_reuse_result <- list(
    reusable_videos = data.frame(),
    accounts_to_fetch = if(exists("coord_users")) coord_users else character(),
    accounts_reused = character(),
    fetch_date_range = list(start = startdate, end = enddate),
    api_calls_saved = 0
  )
}

```

```{r api_functions, include=FALSE, message=FALSE}

# ===== DEFINE ALL API FUNCTIONS FIRST =====

# Function to batch process descriptions and get videos
batch_fetch_videos <- function(descriptions, start_date, end_date, batch_size = 3, max_pages = 3) {
  if (!should_continue_processing()) {
    log_warn("Skipping batch fetch due to quota limits")
    return(data.frame())
  }
  
  log_info(sprintf("Starting batched description queries with batch size %d", batch_size))
  
  # Split descriptions into batches
  batches <- split(descriptions, ceiling(seq_along(descriptions)/batch_size))
  
  log_info(sprintf("Created %d batches from %d unique descriptions", length(batches), length(descriptions)))
  
  # Initialize progress bar
  pb <- progress_bar$new(
    format = "[:bar] :percent :etas",
    total = length(batches),
    clear = FALSE
  )
  
  # Initialize results
  all_results <- data.frame()
  
  # Process each batch
  for (i in seq_along(batches)) {
    if (!should_continue_processing()) {
      log_warn("Stopping batch processing due to quota limits")
      break
    }
    
    current_batch <- batches[[i]]
    batch_count <- length(current_batch)
    
    log_info(sprintf("Processing batch %d/%d with %d descriptions", 
                    i, length(batches), batch_count))
    
    if (batch_count == 0) next
    
    # Try batch query if more than one description
    if (batch_count > 1) {
      tryCatch({
        # CORRECTED: Use safe_api_call for batch queries
        batch_results <- safe_api_call(function() {
          # Build OR conditions for each description with CORRECTED structure
          query_obj <- query()
          
          # CORRECTED: Add OR conditions with proper field_values structure
          or_conditions <- lapply(current_batch, function(desc) {
            list(
              operation = "EQ",
              field_name = "keyword",
              field_values = c(desc)  # ✅ CORRECTED: Use c(desc) not list(desc)
            )
          })
          
          # Set the OR conditions in the query
          query_obj$query$or <- or_conditions
          
          # Execute batch query
          log_info(sprintf("Executing batch query for %d descriptions", batch_count))
          tt_search_api(
            query_obj,
            start_date = start_date,
            end_date = end_date,
            max_pages = max_pages,
            verbose = TRUE
          )
        })
        
        # Process results
        if (is.data.frame(batch_results) && nrow(batch_results) > 0) {
          log_info(sprintf("Batch query returned %d videos", nrow(batch_results)))
          all_results <- bind_rows(all_results, batch_results)
          
          # Log success with sample video IDs
          sample_ids <- if(nrow(batch_results) > 3) {
            paste(batch_results$video_id[1:3], collapse=", ")
          } else if(nrow(batch_results) > 0) {
            paste(batch_results$video_id, collapse=", ")
          } else {
            "none"
          }
          
          log_info(sprintf("Added %d videos from batch (sample video_ids: %s)", 
                          nrow(batch_results), sample_ids))
        } else {
          log_info("Batch query returned no results")
        }
        
      }, error = function(e) {
        if (grepl("429|rate_limit_exceeded|daily_quota_limit_exceeded", conditionMessage(e))) {
          log_error(sprintf("Rate limit exceeded in batch %d: %s - stopping processing", i, conditionMessage(e)))
          global_error_list <<- c(global_error_list, list(list(
            type = "rate_limit_batch",
            batch = i,
            message = conditionMessage(e)
          )))
          # Stop processing and return what we have
          return(all_results)
        } else {
          log_warn(sprintf("Batch query failed: %s - falling back to individual queries", 
                          conditionMessage(e)))
          global_error_list <<- c(global_error_list, list(list(
            type = "batch_query_error",
            batch = i,
            message = conditionMessage(e)
          )))
        }
      })
    }
    
    # For individual queries or fallback
    found_descriptions <- unique(all_results$video_description)
    
    for (desc in current_batch) {
      if (!should_continue_processing()) {
        log_warn("Stopping individual queries due to quota limits")
        break
      }
      
      # Only query if this description isn't already in our results
      if (!desc %in% found_descriptions) {
        tryCatch({
          log_info(sprintf("Individual query for: %s...", substr(desc, 1, 30)))
          videos <- get_videos_by_description(desc, start_date, end_date)
          
          # Add results if any
          if (is.data.frame(videos) && nrow(videos) > 0) {
            all_results <- bind_rows(all_results, videos)
            
            sample_ids <- if(nrow(videos) > 3) {
              paste(videos$video_id[1:3], collapse=", ")
            } else {
              paste(videos$video_id, collapse=", ")
            }
            
            log_info(sprintf("Added %d videos from individual query (sample video_ids: %s)", 
                            nrow(videos), sample_ids))
          }
          
          # Add delay between individual requests
          Sys.sleep(1)
        }, error = function(e) {
          if (grepl("429|rate_limit_exceeded|daily_quota_limit_exceeded", conditionMessage(e))) {
            log_error("Rate limit exceeded during individual queries - stopping processing")
            global_error_list <<- c(global_error_list, list(list(
              type = "rate_limit_individual",
              description = substr(desc, 1, 30),
              message = conditionMessage(e)
            )))
            break
          } else {
            log_warn(sprintf("Individual query failed for description: %s", conditionMessage(e)))
          }
        })
      }
    }
    
    # Update progress bar
    pb$tick()
    
    # Add longer pause between batches to respect rate limits
    Sys.sleep(2)
  }
  
  return(all_results)
}

# Updated get_videos_by_description function with enhanced error handling
get_videos_by_description <- function(desc, start_date, end_date) {
  log_info(sprintf("Starting individual query for description: %s...", substr(desc, 1, 30)))
  
  # First page with safe API call
  first_page_result <- safe_api_call(function() {
    query() |>
      query_and(
        field_name = "keyword",
        operation = "EQ",
        field_values = c(desc)  # ✅ CORRECTED: Use c(desc) not list(desc)
      ) |>
      tt_search_api(
        start_date = start_date,
        end_date = end_date,
        max_pages = 1
      )
  })
  
  # Log using video_id instead of description
  if (is.data.frame(first_page_result) && nrow(first_page_result) > 0) {
    sample_video_id <- first_page_result$video_id[1]
    log_info(sprintf("Retrieved first page with %d results (sample video_id: %s)", 
                    nrow(first_page_result), sample_video_id))
  } else {
    log_info("Retrieved first page with 0 results")
    return(data.frame())
  }
  
  # Try to get more pages with enhanced error handling
  tryCatch({
    full_results <- safe_api_call(function() {
      query() |>
        query_and(
          field_name = "keyword",
          operation = "EQ",
          field_values = c(desc)  # ✅ CORRECTED: Use c(desc) not list(desc)
        ) |>
        tt_search_api(
          start_date = start_date,
          end_date = end_date,
          max_pages = 3  # Conservative
        )
    })
    
    # If pagination succeeded, use the full results
    if (is.data.frame(full_results) && nrow(full_results) > 0) {
      log_info(sprintf("Successfully retrieved all pages with %d total results", 
                      nrow(full_results)))
      return(full_results)
    }
  }, error = function(e) {
    if (grepl("400|invalid_params|Search Id.*invalid or expired", conditionMessage(e))) {
      log_warn("Pagination failed with 400 error - keeping first page results")
      global_error_list <<- c(global_error_list, list(list(
        type = "pagination_error",
        query_type = "description",
        message = conditionMessage(e)
      )))
    } else if (grepl("daily_quota_limit_exceeded", conditionMessage(e))) {
      log_error("Daily quota exceeded during pagination - keeping first page results")
      global_error_list <<- c(global_error_list, list(list(
        type = "daily_quota_pagination",
        query_type = "description",
        message = conditionMessage(e)
      )))
    } else {
      log_error(sprintf("Unexpected error during pagination: %s", conditionMessage(e)))
    }
  })
  
  return(first_page_result)
}

# Function to get videos using traktok package with a single request per chunk approach
get_recent_videos <- function(usernames, start_date, end_date, chunk_size = 20, max_pages = 3) {
  if (!should_continue_processing()) {
    log_warn("Skipping recent videos fetch due to quota limits")
    return(list(data = data.frame(), summary = list(total_videos = 0, failed_chunks = 0, total_chunks = 0)))
  }
  
  # Split usernames into smaller chunks
  username_chunks <- split(usernames, ceiling(seq_along(usernames) / chunk_size))
  total_chunks <- length(username_chunks)
  
  # Initialize results
  all_results <- data.frame()
  failure_count <- 0
  
  # Create progress bar
  pb <- progress_bar$new(
    format = "[:bar] :percent :etas",
    total = total_chunks,
    clear = FALSE
  )
  
  # Process each chunk with enhanced error handling
  for (i in seq_along(username_chunks)) {
    if (!should_continue_processing()) {
      log_warn("Stopping chunk processing due to quota limits")
      break
    }
    
    log_info(sprintf("Processing chunk %d of %d", i, total_chunks))
    
    # Get current chunk
    chunk <- as.character(unlist(username_chunks[i]))
    
    tryCatch({
      # Make request with safe API call
      chunk_result <- safe_api_call(function() {
        query() |>
          query_and(
            field_name = "username",
            operation = "IN",
            field_values = chunk  # This is already correct for username queries
          ) |>
          tt_search_api(
            start_date = start_date,
            end_date = end_date,
            max_pages = max_pages,
            verbose = TRUE,
            fields = "all"
          )
      })
      
      # Log success
      if (is.data.frame(chunk_result) && nrow(chunk_result) > 0) {
        log_info(sprintf("Successfully retrieved %d videos from chunk %d", 
                       nrow(chunk_result), i))
        
        # Add results to main dataframe
        all_results <- bind_rows(all_results, chunk_result)
      } else {
        log_info(sprintf("No videos found for chunk %d", i))
        failure_count <- failure_count + 1
        global_error_list <<- c(global_error_list, list(list(
          type = "empty_chunk",
          chunk = i,
          usernames = chunk
        )))
      }
    }, error = function(e) {
      if (grepl("daily_quota_limit_exceeded", conditionMessage(e))) {
        log_error("Daily quota exceeded - stopping chunk processing")
        failure_count <<- failure_count + 1
        break
      } else {
        log_error(sprintf("Error in chunk %d: %s", i, conditionMessage(e)))
        log_error(sprintf("Usernames in failed chunk: %s", paste(chunk, collapse = ", ")))
        failure_count <<- failure_count + 1
        global_error_list <<- c(global_error_list, list(list(
          type = "chunk_error",
          chunk = i,
          message = conditionMessage(e),
          usernames = chunk
        )))
      }
    })
    
    # Update progress bar
    pb$tick()
    
    # Add longer delay between chunks to respect rate limits
    Sys.sleep(3)
  }
  
  # Final summary
  summary_msg <- sprintf("Collection completed:\n- Total videos: %d\n- Failed chunks: %d/%d",
                       nrow(all_results), failure_count, total_chunks)
  log_info(summary_msg)
  
  if (nrow(all_results) > 0) {
    # Check if author_name exists, fallback to username for compatibility
    user_column <- if("author_name" %in% colnames(all_results)) "author_name" else "username"
    log_info(sprintf("Unique users in results: %d", length(unique(all_results[[user_column]]))))
    log_info(sprintf("Date range: %s to %s", 
                    min(as.Date(all_results$create_time)), 
                    max(as.Date(all_results$create_time))))
  }
  
  return(list(
    data = all_results,
    summary = list(
      total_videos = nrow(all_results),
      failed_chunks = failure_count,
      total_chunks = total_chunks
    )
  ))
}

```

```{r fetch_recent_videos_with_smart_reuse, include=FALSE, message=FALSE, eval=!params$load_from_snapshot}

# Enhanced main fetch function with 23-hour cache check
fetch_recent_videos_smart <- function() {
  if (!params$load_from_snapshot) {
    log_info("=== STARTING SMART VIDEO COLLECTION WITH 23-HOUR CACHE CHECK ===")
    
    # FIRST: Check if we can use the cached data
    if (is_recent_videos_fresh()) {
      log_info("🎯 Using cached recent videos (less than 23 hours old)")
      
      cached_videos <- load_recent_videos_from_cache()
      
      if (!is.null(cached_videos) && nrow(cached_videos) > 0) {
        log_info(sprintf("✅ Successfully loaded %d videos from cache", nrow(cached_videos)))
        
        # Update daily snapshot with cached data
        recent_videos_path <- file.path(daily_output_dir, "recent_videos_snapshot.csv")
        tryCatch({
          # Add back metadata for daily snapshot
          cached_with_metadata <- cached_videos %>%
            mutate(
              fetch_timestamp = as.character(Sys.time()),
              fetch_date = as.character(Sys.Date()),
              data_source = "loaded_from_23h_cache"
            )
          
          readr::write_csv(cached_with_metadata, recent_videos_path)
          log_info(sprintf("✅ Copied cached data to daily snapshot: %s", recent_videos_path))
        }, error = function(e) {
          log_warn(sprintf("Failed to update daily snapshot: %s", e$message))
        })
        
        return(cached_videos)
      } else {
        log_warn("Cache load failed, proceeding with fresh fetch")
      }
    }
    
    # SECOND: If cache is stale or doesn't exist, proceed with smart reuse logic
    log_info("📊 Cache is stale or unavailable - proceeding with smart fetch")
    
    # Check if we have smart reuse results from yesterday
    if (!exists("smart_reuse_result")) {
      log_warn("Smart reuse not available, falling back to normal fetch")
      return(fetch_recent_videos())
    }
    
    # Initialize combined results with reusable videos
    combined_results <- smart_reuse_result$reusable_videos
    
    # If we have accounts that need fresh fetching
    if (length(smart_reuse_result$accounts_to_fetch) > 0) {
      log_info(sprintf("Fetching fresh data for %d accounts", 
                      length(smart_reuse_result$accounts_to_fetch)))
      
      # Fetch fresh videos for accounts not covered by yesterday's data
      fresh_videos_result <- get_recent_videos(
        usernames = smart_reuse_result$accounts_to_fetch,
        start_date = smart_reuse_result$fetch_date_range$start,
        end_date = smart_reuse_result$fetch_date_range$end,
        chunk_size = 20,
        max_pages = 3
      )
      
      fresh_videos <- fresh_videos_result$data
      
      # Ensure data type consistency before combining
      if (nrow(fresh_videos) > 0) {
        # Standardize both datasets to ensure compatibility
        combined_results <- standardize_data_types(combined_results, "reused")
        fresh_videos <- standardize_data_types(fresh_videos, "fresh")
        
        combined_results <- bind_rows(combined_results, fresh_videos)
        log_info(sprintf("Combined %d reused + %d fresh = %d total recent videos", 
                        nrow(smart_reuse_result$reusable_videos),
                        nrow(fresh_videos),
                        nrow(combined_results)))
      } else {
        log_info(sprintf("No fresh videos found, using %d reused videos", 
                        nrow(combined_results)))
      }
    } else {
      log_info(sprintf("Using only %d reused videos, no fresh fetch needed", 
                      nrow(combined_results)))
    }
    
    # Remove duplicates based on video_id
    if (nrow(combined_results) > 0) {
      initial_count <- nrow(combined_results)
      combined_results <- combined_results %>% 
        distinct(video_id, .keep_all = TRUE)
      final_count <- nrow(combined_results)
      
      if (initial_count != final_count) {
        log_info(sprintf("Removed %d duplicate videos", initial_count - final_count))
      }
      
      # Log final user count with proper column detection
      user_column <- if("author_name" %in% colnames(combined_results)) "author_name" else "username"
      log_info(sprintf("Final unique users: %d", length(unique(combined_results[[user_column]]))))
    }
    
    # === IMMEDIATE STORAGE OF RECENT VIDEOS ===
    if (nrow(combined_results) > 0) {
      # Create storage paths
      recent_videos_path <- file.path(daily_output_dir, "recent_videos_snapshot.csv")
      
      # Store with metadata about the fetch
      recent_videos_with_metadata <- combined_results %>%
        mutate(
          fetch_timestamp = as.character(Sys.time()),
          fetch_date = as.character(Sys.Date()),
          data_source = case_when(
            video_id %in% smart_reuse_result$reusable_videos$video_id ~ "reused_from_yesterday",
            TRUE ~ "fresh_api_fetch"
          )
        )
      
      # Save recent videos immediately
      tryCatch({
        readr::write_csv(recent_videos_with_metadata, recent_videos_path)
        log_info(sprintf("✅ STORED recent videos to: %s", recent_videos_path))
        log_info(sprintf("   - Total videos: %d", nrow(recent_videos_with_metadata)))
        log_info(sprintf("   - Reused: %d, Fresh: %d", 
                        sum(recent_videos_with_metadata$data_source == "reused_from_yesterday"),
                        sum(recent_videos_with_metadata$data_source == "fresh_api_fetch")))
      }, error = function(e) {
        log_error(sprintf("Failed to store recent videos: %s", e$message))
      })
      
      # CRITICAL: Update the recent_videos_latest.csv file (23-hour cache)
      backup_path <- file.path(params$base_dir, "output", "recent_videos_latest.csv")
      tryCatch({
        readr::write_csv(recent_videos_with_metadata, backup_path)
        log_info(sprintf("✅ UPDATED 23-hour cache: %s", backup_path))
        log_info("⏰ This cache will be used for the next 23 hours")
      }, error = function(e) {
        log_error(sprintf("❌ Failed to update 23-hour cache: %s", e$message))
      })
    } else {
      log_warn("No recent videos to store")
    }
    
    log_info(sprintf("=== SMART COLLECTION COMPLETED ==="))
    log_info(sprintf("Total videos: %d", nrow(combined_results)))
    log_info(sprintf("API calls saved: %d", smart_reuse_result$api_calls_saved))
    
    return(combined_results)
  } else {
    log_info("Loading from snapshot, skipping video collection")
    return(NULL)
  }
}

# Fallback fetch function (when smart reuse is not available)
fetch_recent_videos <- function() {
  if (!params$load_from_snapshot) {
    log_info("Starting video collection (fallback mode)")
    
    # Check 23-hour cache first even in fallback mode
    if (is_recent_videos_fresh()) {
      log_info("🎯 Using cached recent videos in fallback mode")
      cached_videos <- load_recent_videos_from_cache()
      if (!is.null(cached_videos) && nrow(cached_videos) > 0) {
        return(cached_videos)
      }
    }
    
    # Proceed with fresh fetch
    recent_videos_result <- get_recent_videos(
      usernames = coord_users,
      start_date = startdate,
      end_date = enddate,
      chunk_size = 20,
      max_pages = 3
    )
    recent_videos <- recent_videos_result$data
    
    # === IMMEDIATE STORAGE FOR FALLBACK MODE ===
    if (nrow(recent_videos) > 0) {
      recent_videos_path <- file.path(daily_output_dir, "recent_videos_snapshot.csv")
      
      recent_videos_with_metadata <- recent_videos %>%
        mutate(
          fetch_timestamp = as.character(Sys.time()),
          fetch_date = as.character(Sys.Date()),
          data_source = "fallback_full_fetch"
        )
      
      # Save to daily snapshot
      tryCatch({
        readr::write_csv(recent_videos_with_metadata, recent_videos_path)
        log_info(sprintf("✅ STORED recent videos (fallback) to: %s", recent_videos_path))
      }, error = function(e) {
        log_error(sprintf("Failed to store recent videos (fallback): %s", e$message))
      })
      
      # CRITICAL: Update the 23-hour cache
      backup_path <- file.path(params$base_dir, "output", "recent_videos_latest.csv")
      tryCatch({
        readr::write_csv(recent_videos_with_metadata, backup_path)
        log_info(sprintf("✅ UPDATED 23-hour cache (fallback): %s", backup_path))
      }, error = function(e) {
        log_error(sprintf("❌ Failed to update 23-hour cache (fallback): %s", e$message))
      })
    }
    
    log_info(sprintf("Video collection completed with %d videos", nrow(recent_videos)))
    return(recent_videos)
  } else {
    log_info("Loading from snapshot, skipping video collection")
    return(NULL)
  }
}

# Replace the original fetch call
if (exists("coord_users")) {
  recent_videos <- fetch_recent_videos_smart()
  
  # Log storage completion
  if (!is.null(recent_videos) && nrow(recent_videos) > 0) {
    log_info("🎯 RECENT VIDEOS STORAGE COMPLETED - Ready for next run's smart reuse")
  }
}

```

```{r load_recent_videos_for_reuse, include=FALSE, message=FALSE}

# Function to load recent videos from previous runs for smart reuse
load_recent_videos_for_reuse <- function() {
  log_info("=== LOADING RECENT VIDEOS FOR SMART REUSE ===")
  
  # Try to load recent videos from yesterday first
  yesterday_paths <- get_yesterday_data_path()
  recent_videos_yesterday <- file.path(yesterday_paths$dir, "recent_videos_snapshot.csv")
  
  if (file.exists(recent_videos_yesterday)) {
    tryCatch({
      yesterday_recent <- readr::read_csv(recent_videos_yesterday, 
                                        col_types = cols(
                                          video_id = col_character(),
                                          create_time = col_character(),
                                          fetch_timestamp = col_character(),
                                          fetch_date = col_character(),
                                          data_source = col_character(),
                                          effect_ids = col_character(),
                                          music_id = col_character(),
                                          hashtag_names = col_character()
                                        ))
      
      log_info(sprintf("✅ Loaded %d recent videos from yesterday (%s)", 
                      nrow(yesterday_recent), yesterday_paths$date))
      
      return(yesterday_recent)
    }, error = function(e) {
      log_warn(sprintf("Failed to load yesterday's recent videos: %s", e$message))
    })
  }
  
  # Fallback to latest backup
  backup_path <- file.path(params$base_dir, "output", "recent_videos_latest.csv")
  if (file.exists(backup_path)) {
    tryCatch({
      backup_recent <- readr::read_csv(backup_path,
                                     col_types = cols(
                                       video_id = col_character(),
                                       create_time = col_character(),
                                       fetch_timestamp = col_character(),
                                       fetch_date = col_character(),
                                       data_source = col_character(),
                                       effect_ids = col_character(),
                                       music_id = col_character(),
                                       hashtag_names = col_character()
                                     ))
      
      log_info(sprintf("✅ Loaded %d recent videos from backup", nrow(backup_recent)))
      return(backup_recent)
    }, error = function(e) {
      log_warn(sprintf("Failed to load backup recent videos: %s", e$message))
    })
  }
  
  log_info("No previous recent videos found for reuse")
  return(data.frame())
}

# Enhanced function to filter recent videos for current monitoring scope
filter_recent_videos_for_monitoring <- function(previous_recent_videos, coord_users, start_date, end_date) {
  if (is.null(previous_recent_videos) || nrow(previous_recent_videos) == 0) {
    return(data.frame())
  }
  
  # Filter for current monitoring scope
  relevant_recent <- previous_recent_videos %>%
    filter(
      # Only videos from accounts we're currently monitoring
      author_name %in% coord_users,
      # Only videos within our current date range
      as.Date(create_time) >= as.Date(start_date),
      as.Date(create_time) <= as.Date(end_date)
    )
  
  if (nrow(relevant_recent) > 0) {
    log_info(sprintf("📋 Filtered to %d relevant recent videos for current monitoring scope", 
                    nrow(relevant_recent)))
    
    # Remove the metadata columns that were added during storage
    relevant_recent <- relevant_recent %>%
      select(-any_of(c("fetch_timestamp", "fetch_date", "data_source")))
  }
  
  return(relevant_recent)
}

# Execute loading and filtering if we're not loading from snapshot
if (!params$load_from_snapshot && exists("coord_users") && exists("startdate") && exists("enddate")) {
  # Load previous recent videos
  previous_recent_videos <- load_recent_videos_for_reuse()
  
  # Filter for current monitoring scope
  filtered_recent_videos <- filter_recent_videos_for_monitoring(
    previous_recent_videos, 
    coord_users, 
    startdate, 
    enddate
  )
  
  # Store for potential use in smart reuse logic
  assign("previous_recent_videos_filtered", filtered_recent_videos, envir = .GlobalEnv)
  
  log_info("🔄 Recent videos loading for smart reuse completed")
}

```

```{r fetch_all_videos_smart, include=FALSE, message=FALSE, eval=!params$load_from_snapshot}

# Enhanced fetch_all_videos function that leverages smart reuse and incremental fetching
fetch_all_videos_smart <- function(recent_videos, overall_start_date, overall_end_date) { # Renamed params for clarity
 if (!should_continue_processing()) {
   log_warn("Skipping all videos fetch due to quota limits")
   return(data.frame())
 }

 # Helper function to safely log descriptions
 safe_log_desc <- function(desc, ...) {
   # Sanitize description for logging - remove ALL problematic characters
   safe_desc <- if(is.character(desc) && !is.na(desc)) {
     # Remove characters that cause issues with logging
     desc %>%
       stringr::str_replace_all("[{}']", "") %>%  # Remove braces and quotes
       stringr::str_replace_all("[@←→♥✪\\\\]", "") %>% # Remove special symbols
       stringr::str_replace_all("[\r\n\t]", " ") %>% # Replace newlines/tabs  
       stringr::str_replace_all("\\s+", " ") %>% # Collapse multiple spaces
       stringr::str_trim() %>% # Trim whitespace
       stringr::str_trunc(30, "right", ellipsis = "...") # Truncate
   } else {
     "INVALID_DESC"
   }
   
   # Build message
   message_parts <- list(...)
   full_message <- if (length(message_parts) > 0) {
     paste0("Desc: ", safe_desc, ": ", paste(message_parts, collapse = " "))
   } else {
     paste0("Processing desc: ", safe_desc)
   }
   
   # Use basic log_info with cleaned message
   log_info(full_message)
 }

 # 1. FIRST: Identify Target Descriptions from recent_videos
 if (is.null(recent_videos) || nrow(recent_videos) == 0) {
   log_warn("No recent_videos available to identify target descriptions.")
   return(data.frame())
 }

 target_descriptions <- recent_videos %>%
   filter(!is.na(video_description) & nchar(video_description) >= 80) %>%
   pull(video_description) %>%
   unique()

 if (length(target_descriptions) == 0) {
   log_info("No target descriptions found in recent_videos meeting criteria.")
   return(data.frame())
 }
 log_info(paste0("Identified ", length(target_descriptions), " unique target descriptions from recent_videos."))

 # 2. NOW: Load Yesterday's Data filtered by target descriptions
 base_videos <- data.frame() # Initialize as empty
 yesterday_paths <- get_yesterday_data_path() # Assumes this function exists

 if (file.exists(yesterday_paths$snapshot)) {
   tryCatch({
     # Define expected columns and their types for robustness
     expected_col_types <- cols(
       video_id = col_character(),
       create_time = col_character(), # Will be converted to POSIXct/Date later
       video_description = col_character(),
       author_id = col_character(), # Or author_unique_id
       author_name = col_character(), # Display name
       username = col_character(), # Often the unique @username
       region_code = col_character(),
       video_duration = col_double(),
       height = col_double(),
       width = col_double(),
       effect_ids = col_character(), # Often lists, converted to string by standardize
       music_id = col_character(),   # Often lists, converted to string by standardize
       hashtag_names = col_character(), # Often lists, converted to string by standardize
       playlist_id = col_character(),
       voice_to_text = col_character(),
       view_count = col_double(), # Use double for flexibility with large numbers or NAs
       like_count = col_double(),
       comment_count = col_double(),
       share_count = col_double(),
       video_title = col_character(), # If you have this
       # Add any other columns present in your snapshot
       .default = col_character() # Fallback for any unspecified columns
     )

     yesterday_full_data <- readr::read_csv(yesterday_paths$snapshot, col_types = expected_col_types)
     
     # Ensure create_time is character format for consistency before standardization
     if ("create_time" %in% colnames(yesterday_full_data)) {
       yesterday_full_data <- yesterday_full_data %>%
         mutate(create_time = as.character(create_time))
     }

     yesterday_full_data <- standardize_data_types(yesterday_full_data, "yesterday_full_snapshot") # standardize

     # CORRECTED: Filter yesterday's data by BOTH date range AND target descriptions
     if (nrow(yesterday_full_data) > 0 && 
         "create_time" %in% colnames(yesterday_full_data) && 
         "video_description" %in% colnames(yesterday_full_data)) {
       
       base_videos <- yesterday_full_data %>%
         filter(
           as.Date(create_time) >= as.Date(overall_start_date),
           as.Date(create_time) <= as.Date(overall_end_date),
           video_description %in% target_descriptions  # ADDED: Only keep matching descriptions
         )
       
       log_info(paste0("Loaded ", nrow(base_videos), " videos from yesterday's cache matching ", 
                      length(target_descriptions), " target descriptions within date range"))
     } else {
       base_videos <- data.frame() # Ensure it's an empty df if no relevant data
     }
     
   }, error = function(e) {
     log_warn(paste0("Could not load or process yesterday's full data: ", e$message, ". Starting with empty base_videos."))
     base_videos <- data.frame() # Ensure it's an empty df on error
   })
 } else {
   log_info("No yesterday's snapshot found. Starting with an empty base_videos.")
   base_videos <- data.frame()
 }

 # 3. Determine API Query Parameters for Each Target Description (Incremental Logic)
 descriptions_to_query_api <- list() # list of lists: list(description = "...", query_start_date = Date_object)

 for (desc_val in target_descriptions) {
   if (!should_continue_processing()) break

   # Videos from cache for this specific description
   cached_videos_for_this_desc <- data.frame()
   if (nrow(base_videos) > 0 && "video_description" %in% colnames(base_videos) && "create_time" %in% colnames(base_videos)) {
     cached_videos_for_this_desc <- base_videos %>%
       filter(video_description == desc_val & !is.na(create_time))
   }

   api_query_start_date_for_this_desc <- as.Date(overall_start_date) # Default

   if (nrow(cached_videos_for_this_desc) > 0) {
     max_cached_create_time_posix <- cached_videos_for_this_desc %>%
       mutate(create_time_dt = lubridate::as_datetime(create_time)) %>% # robust parsing
       filter(!is.na(create_time_dt)) %>%
       pull(create_time_dt) %>%
       max(na.rm = TRUE)

     if (is.finite(max_cached_create_time_posix)) {
       # Query from the day AFTER the last cached video for this description
       next_day_to_query <- as.Date(max_cached_create_time_posix) + lubridate::days(1)
       # Ensure we don't go earlier than the overall_start_date for the current run
       api_query_start_date_for_this_desc <- max(next_day_to_query, as.Date(overall_start_date))
       
       # Use safe logging function
       safe_log_desc(desc_val, 
                    "Max cached date", format(as.Date(max_cached_create_time_posix), "%Y-%m-%d"),
                    ". New query start for API:", format(api_query_start_date_for_this_desc, "%Y-%m-%d"))
     } else {
       safe_log_desc(desc_val, 
                    "No valid max_cached_create_time. Defaulting query start to overall_start_date:",
                    format(api_query_start_date_for_this_desc, "%Y-%m-%d"))
     }
   } else {
     safe_log_desc(desc_val, 
                  "Not found in cache. Querying from overall_start_date:",
                  format(api_query_start_date_for_this_desc, "%Y-%m-%d"))
   }

   # Only add to API query list if the calculated start date is not after the overall end date
   if (api_query_start_date_for_this_desc <= as.Date(overall_end_date)) {
     descriptions_to_query_api[[length(descriptions_to_query_api) + 1]] <- list(
       description = desc_val,
       query_start_date = api_query_start_date_for_this_desc # Stored as Date object
     )
   } else {
     safe_log_desc(desc_val, 
                  "Calculated API query start date", format(api_query_start_date_for_this_desc, "%Y-%m-%d"),
                  "is after overall_end_date", format(as.Date(overall_end_date), "%Y-%m-%d"), ". Skipping API call.")
   }
 } # End loop over target_descriptions

 # 4. Perform API Calls for Descriptions Needing Updates
 newly_fetched_videos_list <- list()
 if (length(descriptions_to_query_api) > 0) {
   log_info(paste0("Need to query API for ", length(descriptions_to_query_api), " description/date-range pairs."))
   
   # OPTION 1: Iterate and call get_videos_by_description (simpler, potentially slower)
   # OPTION 2: Modify batch_fetch_videos to handle list of (desc, start_date)
   # For now, using Option 1 for direct replacement.

   pb_desc <- progress::progress_bar$new(
     format = "Fetching incremental descriptions [:bar] :percent :eta",
     total = length(descriptions_to_query_api), clear = FALSE, width = 60
   )

   for (query_item in descriptions_to_query_api) {
     pb_desc$tick()
     if (!should_continue_processing()) {
       log_warn("Stopping incremental description fetch due to quota limits.")
       break
     }
     
     current_api_query_start_date <- query_item$query_start_date
     current_api_query_end_date <- as.Date(overall_end_date)

     # Use safe logging for API call description
     tryCatch({
       safe_log_desc(query_item$description, 
                    "API Start:", format(current_api_query_start_date, "%Y-%m-%d"),
                    ", API End:", format(current_api_query_end_date, "%Y-%m-%d"))
     }, error = function(e) {
       log_info(paste0("API Call for description (logging failed): API Start: ", 
                      format(current_api_query_start_date, "%Y-%m-%d"), 
                      ", API End: ", format(current_api_query_end_date, "%Y-%m-%d")))
     })
     
     tryCatch({
         # get_videos_by_description expects Date objects or YYYY-MM-DD strings
         videos_from_api_call <- get_videos_by_description(
                             query_item$description,
                             start_date = current_api_query_start_date, 
                             end_date = current_api_query_end_date 
                         )
         if (is.data.frame(videos_from_api_call) && nrow(videos_from_api_call) > 0) {
             newly_fetched_videos_list[[length(newly_fetched_videos_list) + 1]] <- standardize_data_types(videos_from_api_call, "api_incremental_desc")
         }
     }, error = function(e) {
         # Safe error logging without problematic description formatting
         log_error(paste0("Error fetching for description: ", e$message))
         # update_quota_status is handled within safe_api_call (used by get_videos_by_description)
     })
     Sys.sleep(1.5) # Slightly longer delay between individual description API calls
   } # End loop over descriptions_to_query_api
 } else {
   log_info("No new API calls needed for descriptions based on incremental logic.")
 }

 all_newly_fetched_videos_df <- data.frame()
 if (length(newly_fetched_videos_list) > 0) {
   all_newly_fetched_videos_df <- dplyr::bind_rows(newly_fetched_videos_list)
   log_info(paste0("Fetched a total of ", nrow(all_newly_fetched_videos_df), " new videos via API for descriptions."))
 }

 # 5. Combine Cached Videos with Newly Fetched Videos
 # Note: base_videos is already filtered to only include videos matching target_descriptions
 combined_videos <- dplyr::bind_rows(base_videos, all_newly_fetched_videos_df)

 # 6. Final Processing, Deduplication, and Saving
 if (nrow(combined_videos) > 0) {
   final_results <- combined_videos %>%
     # Ensure create_time is character for final CSV, or handle NAs if it became POSIXct
     mutate(create_time = as.character(create_time)) %>% 
     mutate(
       region_code = if("region_code" %in% colnames(.)) toupper(region_code) else NA_character_,
       video_url = if("author_name" %in% colnames(.) && "video_id" %in% colnames(.)) {
         paste0("https://www.tiktok.com/@", author_name, "/video/", video_id)
       } else {
         NA_character_
       }
     ) %>%
     # Filter out rows where video_description might be NA or empty, if this column is critical
     filter(if("video_description" %in% colnames(.)) (!is.na(video_description) & video_description != "") else TRUE) %>%
     # Crucial deduplication by video_id, keeping all columns from the first occurrence
     distinct(video_id, .keep_all = TRUE)

   log_info("=== INCREMENTAL 'ALL VIDEOS' COLLECTION COMPLETED ===")
   log_info(paste0("Base videos (reused from cache, matching target descriptions): ", nrow(base_videos)))
   log_info(paste0("Fresh API videos (incrementally fetched for descriptions): ", nrow(all_newly_fetched_videos_df)))
   log_info(paste0("Total unique videos after combination and deduplication: ", nrow(final_results)))

   user_column_name <- if("author_name" %in% colnames(final_results)) "author_name" else if("username" %in% colnames(final_results)) "username" else "video_id" # Fallback
   if (user_column_name != "video_id" || !"author_name" %in% colnames(final_results)) { # Check if a reasonable user column exists
       log_info(paste0("Unique users (using '", user_column_name, "'): ", n_distinct(final_results[[user_column_name]])))
   }
   if ("video_description" %in% colnames(final_results)) {
       log_info(paste0("Unique descriptions in final set: ", n_distinct(final_results$video_description)))
   }
   
   # Save results (ensure params$base_dir and daily_output_dir are accessible)
   output_dir_main <- file.path(params$base_dir, "output")
   dir.create(output_dir_main, showWarnings = FALSE, recursive = TRUE)

   latest_snapshot_file <- file.path(output_dir_main, "latest_snapshot.csv")
   # daily_output_dir should be defined globally from create_daily_output_dir()
   daily_snapshot_file_path <- file.path(daily_output_dir, "daily_snapshot.csv") 
   
   readr::write_csv(final_results, latest_snapshot_file)
   readr::write_csv(final_results, daily_snapshot_file_path)
   log_info(paste0("Results saved to ", latest_snapshot_file, " and ", daily_snapshot_file_path))
   
   return(final_results)
 } else {
   log_warn("No videos found/remained after incremental fetch and combination process.")
   return(data.frame())
 }
}

# Replace the original all videos fetch call (this part remains the same)
if (exists("recent_videos") && (is.null(recent_videos) || is.data.frame(recent_videos))) {
 log_info("Starting smart fetch for all videos with matching descriptions (incremental logic)")
 # Parameters are recent_videos, overall_start_date (global startdate), overall_end_date (global enddate)
 all_videos <- fetch_all_videos_smart(recent_videos, startdate, enddate)
 
 # Assign to global environment
 assign("all_videos", all_videos, envir = .GlobalEnv)
} else {
 log_warn("recent_videos not properly defined, skipping 'all_videos' fetch.")
 # Define all_videos as empty if not fetched to avoid errors downstream
 all_videos <- data.frame() 
 assign("all_videos", all_videos, envir = .GlobalEnv)
}

```

```{r coordinated_detection, include=FALSE, message=FALSE}

# Function to detect coordination patterns
detect_coordination <- function(all_videos) {
  # Define a helper for consistent empty/error returns
  empty_coordination_result <- function(current_coord_users) {
    list(
      coord_graph = NULL,
      summary_groups = data.frame(), 
      summary_accounts = data.frame(), 
      new_accounts = data.frame(),
      updated_list = current_coord_users 
    )
  }

  tryCatch({
    if(nrow(all_videos) > 0 && 
       "video_description" %in% colnames(all_videos) &&
       "author_name" %in% colnames(all_videos) && # Added author_name check
       "video_id" %in% colnames(all_videos) &&
       "create_time" %in% colnames(all_videos)) {
      
      # Prepare data with correct field mappings
      prep_data <- CooRTweet::prep_data(
        x = all_videos,
        object_id = "video_description",
        account_id = "author_name",      # Changed from username to author_name
        content_id = "video_id",         # Already correct
        timestamp_share = "create_time"   # Already correct
      )
      
      log_info("Data prepared for coordination detection")
      
      # Detect coordinated groups
      result <- CooRTweet::detect_groups(
        x = prep_data,
        time_window = params$time_window,
        min_participation = params$min_participation,
        remove_loops = TRUE)
      
      coord_graph <- CooRTweet::generate_coordinated_network(
        x = result, 
        edge_weight = params$edge_weight,
        objects = TRUE,
        subgraph = 1)
      
      log_info("Coordination network generated")
      
      # Calculate summary statistics if graph is valid
      if(!is.null(coord_graph) && !is.null(result)) {
        if(!("igraph" %in% class(coord_graph))) {
          log_error("coord_graph is not of type igraph")
          stop("coord_graph is not of type igraph. Please check the generate_coordinated_network output.")
        }
        
        # Calculate summary statistics
        summary_groups <- CooRTweet::group_stats(
          coord_graph = coord_graph, 
          weight_threshold = "full"
        )
        
        summary_accounts <- CooRTweet::account_stats(
          coord_graph = coord_graph, 
          result = result, 
          weight_threshold = "full"
        )
        
        log_info("Successfully calculated summary statistics")
        
        # Get current list of coordinated accounts from summary_accounts
        current_coordinated_accounts <- summary_accounts$account_id
        log_info(sprintf("Found %d accounts showing coordination in current analysis", 
                        length(current_coordinated_accounts)))
        
        # Get the starting list of known accounts
        log_info(sprintf("Starting with %d known coordinated accounts", 
                        length(coord_users)))
        
        # Identify truly new accounts (not in starting list)
        new_account_ids <- data.frame(
          account_id = setdiff(current_coordinated_accounts, coord_users)
        )
        
        # Create updated list
        updated_list <- unique(c(coord_users, new_account_ids$account_id))
        
        # Log the results
        if(nrow(new_account_ids) > 0) {
          log_info(sprintf("Found %d new coordinated accounts (not in starting list)", 
                          nrow(new_account_ids)))
          log_info(sprintf("Total unique accounts after update: %d", 
                          length(updated_list)))
        } else {
          log_info("No new coordinated accounts found")
        }
        
        # Save results
        dir.create("output/coordination", showWarnings = FALSE, recursive = TRUE)
        
        write_csv(summary_groups, "output/coordination/summary_groups.csv")
        write_csv(summary_accounts, "output/coordination/summary_accounts.csv")
        write_csv(data.frame(account_id = updated_list), 
                  "output/coordination/updated_account_list.csv")
        
        return(list(
          coord_graph = coord_graph,
          summary_groups = summary_groups,
          summary_accounts = summary_accounts,
          new_accounts = new_account_ids,
          updated_list = updated_list
        ))
        
      } else {
        log_warn("coord_graph or result is NULL. Skipping summary statistics.")
        return(empty_coordination_result(coord_users)) # Use helper
      }
    } else {
      log_warn("No videos available or missing required columns for coordinated detection analysis")
      return(empty_coordination_result(coord_users)) # Use helper
    }
  }, error = function(e) {
    log_error(paste("Error in coordinated detection:", e$message))
    global_error_list <<- c(global_error_list, list(list(
      type = "coordinated_detection",
      message = e$message
    )))
    return(empty_coordination_result(coord_users)) # Use helper
  })
}

# Execute if we have all_videos
if (exists("all_videos") && is.data.frame(all_videos) && nrow(all_videos) > 0) {
  log_info("Starting coordination detection analysis")
  coordination_results <- detect_coordination(all_videos)
  
  # Update global variables if needed
  if (exists("coordination_results") && !is.null(coordination_results$updated_list)) {
    coord_users <- coordination_results$updated_list
  }
} else {
  log_warn("No videos available for coordination analysis")
  # Ensure coordination_results is defined even if empty
   coordination_results <- list(
      coord_graph = NULL,
      summary_groups = data.frame(),
      summary_accounts = data.frame(),
      new_accounts = data.frame(),
      updated_list = if(exists("coord_users")) coord_users else character(0)
    )
}

```

```{r label_clusters, include=FALSE, message=FALSE}

# Function to get OpenAI credentials
get_openai_credentials <- function(custom_key = NULL, custom_org = NULL) {
  if (!is.null(custom_key) && custom_key != "default") {
    return(list(
      api_key = custom_key,
      org_id = if (!is.null(custom_org) && custom_org != "default") custom_org else Sys.getenv("OPENAI_VERAAI_ORG_ID")
    ))
  }
  return(list(
    api_key = Sys.getenv("OPENAI_VERAAI_API_KEY"),
    org_id = Sys.getenv("OPENAI_VERAAI_ORG_ID")
  ))
}

# Function to generate cluster labels using OpenAI API
generate_cluster_labels <- function(coordination_results, all_videos) {
  # Get credentials using the custom key if provided
  credentials <- get_openai_credentials(params$openai_key, params$openai_org)
  
  # Check for API credentials
  if (credentials$api_key == "" || credentials$org_id == "") {
    log_warn("OpenAI credentials not set")
    return(NULL)
  }
  
  credentials <- get_openai_credentials(params$openai_key, params$openai_org)
  log_info(sprintf("Using API key: %s...", substr(credentials$api_key, 1, 5)))
  log_info(sprintf("Using org ID: %s", credentials$org_id))
  log_info(sprintf("Model: %s", params$ai_model))
  log_info(sprintf("Request URL: %s", "https://api.openai.com/v1/chat/completions"))
  
  # Function to get cluster content
  get_cluster_content <- function(cluster_id, max_videos = 20) {
    # Get accounts in this cluster
    cluster_accounts <- V(coordination_results$coord_graph)$name[
      components(coordination_results$coord_graph)$membership == cluster_id
    ]
    
    # Get videos from these accounts
    cluster_videos <- all_videos %>%
      filter(author_name %in% cluster_accounts) %>%
      mutate(
        total_engagement = view_count + like_count + share_count + comment_count
      ) %>%
      arrange(desc(total_engagement)) %>%
      head(max_videos)
    
    # Combine descriptions and voice transcripts
    content_parts <- sapply(1:nrow(cluster_videos), function(i) {
      video <- cluster_videos[i,]
      parts <- c()
      if (!is.na(video$video_description)) {
        parts <- c(parts, sprintf("Description: %s", video$video_description))
      }
      if (!is.na(video$voice_to_text)) {
        parts <- c(parts, sprintf("Voice transcript: %s", video$voice_to_text))
      }
      paste(parts, collapse = " | ")
    })
    
    paste(content_parts, collapse = "\n\n")
  }
  
  # Function to get label from OpenAI
  get_cluster_label <- function(content, retries = 3) {
    system_prompt <- "You are a researcher analyzing coordinated behavior on TikTok. 
Your task is to generate a concise (maximum 10 words), descriptive label in English that 
captures the main theme or characteristic of the content shared by a cluster of TikTok accounts.
Focus on the primary topic, political alignment (if evident), or content style that unifies these accounts.
The label should be factual and neutral in tone."
    
    user_prompt <- sprintf("Analyze the following content from a cluster of TikTok accounts and 
generate a concise label that describes their shared characteristics:

%s

Label:", content)
    
    for (attempt in 1:retries) {
      tryCatch({
        response <- httr::POST(
          url = "https://api.openai.com/v1/chat/completions",
          httr::add_headers(
            "Content-Type" = "application/json",
            "Authorization" = paste("Bearer", credentials$api_key),
            "OpenAI-Organization" = credentials$org_id
          ),
          body = list(
            model = params$ai_model,
            messages = list(
              list(role = "system", content = system_prompt),
              list(role = "user", content = user_prompt)
            ),
            temperature = 0,
            max_tokens = 50
          ),
          encode = "json"
        )
        
        if (httr::status_code(response) == 200) {
          content <- httr::content(response)
          return(content$choices[[1]]$message$content)
        } else {
          status_code <- httr::status_code(response)
          log_warn(sprintf("API request attempt %d failed (status: %d)", 
                         attempt, status_code))
        }
      }, error = function(e) {
        log_warn(sprintf("Attempt %d failed: %s", attempt, conditionMessage(e)))
      })
      
      if (attempt < retries) Sys.sleep(2^attempt)
    }
    
    return("Failed to generate label")
  }
  
  # Get number of components
  num_components <- components(coordination_results$coord_graph)$no
  
  # Initialize progress bar
  pb <- progress_bar$new(
    format = "[:bar] :percent Labeling cluster :current/:total",
    total = num_components
  )
  
  # Process each component
  labels <- data.frame(
    component = integer(),
    label = character(),
    stringsAsFactors = FALSE
  )
  
  for (i in 1:num_components) {
    # Get content for this cluster
    content <- get_cluster_content(i)
    
    # Get label
    label <- get_cluster_label(content)
    
    # Add to results
    labels <- rbind(labels, data.frame(
      component = i,
      label = label,
      stringsAsFactors = FALSE
    ))
    
    # Update progress bar
    pb$tick()
    
    # Add small delay
    Sys.sleep(0.5)
  }
  
  # Save labels to file
  write_csv(labels, file.path(daily_output_dir, "cluster_labels.csv"))
  
  return(labels)
}

# Generate labels if we have coordination results
if (exists("coordination_results") && 
    !is.null(coordination_results$coord_graph) && 
    (
      (params$openai_key != "default" && params$openai_org != "default") ||
      (Sys.getenv("OPENAI_VERAAI_API_KEY") != "" && Sys.getenv("OPENAI_VERAAI_ORG_ID") != "")
    )) {
  log_info("Starting cluster labeling")
  cluster_labels <- generate_cluster_labels(coordination_results, all_videos)
  
  # Add labels to clusters table if it exists
  if (!is.null(cluster_labels) && exists("clusters_table")) {
    clusters_table <- clusters_table %>%
      left_join(cluster_labels, by = c("Component" = "component")) %>%
      select(
        Component,
        Label = label,
        everything()
      )
  }
  
  log_info("Cluster labeling completed")
} else {
  log_warn("Skipping cluster labeling - missing coordination results or API credentials")
}

```

```{r save_output, include=FALSE, message=FALSE}

# Create necessary directories
tryCatch({
  dir.create(file.path(params$base_dir, "output"), showWarnings = FALSE, recursive = TRUE)
  dir.create(file.path(params$base_dir, "output", "coordination"), showWarnings = FALSE, recursive = TRUE)
}, error = function(e) {
  log_error(sprintf("Error creating directories: %s", conditionMessage(e)))
})

# Function to update account logs and save results
update_account_logs <- function(coordination_results) {
  today_date <- format(Sys.Date(), "%Y-%m-%d")
  
  # Define file paths
  daily_accounts_file <- file.path(daily_output_dir, "tiktok_coordinated_accounts_ids.csv")
  global_accounts_file <- file.path(params$base_dir, "lists", "tiktok_coordinated_accounts_ids.csv")
  log_file_path <- file.path(daily_output_dir, "new_accounts_log.csv")
  
  # Save daily snapshot
  if (exists("all_videos") && !is.null(all_videos) && is.data.frame(all_videos) && nrow(all_videos) > 0) {
    write_csv(all_videos, file.path(daily_output_dir, "daily_snapshot.csv"))
  } else {
    log_warn("all_videos is NULL, empty, or not a dataframe. Skipping daily_snapshot.csv save.")
  }
  
  tryCatch({
    # Create output directory if it doesn't exist
    dir.create(dirname(log_file_path), showWarnings = FALSE, recursive = TRUE)
    
    # Read existing log file if it exists
    existing_log <- if (file.exists(log_file_path)) {
      read_csv(log_file_path, show_col_types = FALSE)
    } else {
      log_info("Creating new accounts log file")
      data.frame(
        Timestamp = as.Date(character()),
        New_Accounts_Count = numeric(),
        Total_Accounts = numeric()
      )
    }
    
    # Prepare the new log entry with correct column names
    new_entry <- data.frame(
      Timestamp = as.Date(Sys.Date()),
      New_Accounts_Count = if(!is.null(coordination_results$new_accounts)) nrow(coordination_results$new_accounts) else 0,
      Total_Accounts = if(!is.null(coordination_results$updated_list)) length(coordination_results$updated_list) else 0
    )
    
    # Append the new entry to the log file
    write_csv(
      bind_rows(existing_log, new_entry),
      log_file_path
    )
    
    log_info(sprintf("Added new log entry: %d new accounts, %d total accounts", 
                     new_entry$New_Accounts_Count, 
                     new_entry$Total_Accounts))
    
    # Update the coordinated accounts list
    if (!is.null(coordination_results$updated_list) && length(coordination_results$updated_list) > 0) {
      # Save updated list in both the daily folder and the global folder
      write_csv(
        data.frame(x = coordination_results$updated_list),
        daily_accounts_file
      )
      
      write_csv(
        data.frame(x = coordination_results$updated_list),
        global_accounts_file
      )
      
      log_info(sprintf("Successfully updated coordinated accounts list with %d accounts in both locations", 
                       length(coordination_results$updated_list)))
    } else {
      log_warn("No accounts to update in the coordinated accounts list")
    }
    
    return(TRUE)
    
  }, error = function(e) {
    log_error(sprintf("Error updating account logs: %s", conditionMessage(e)))
    global_error_list <<- c(global_error_list, list(list(
      type = "update_logs_error",
      message = e$message
    )))
    return(FALSE)
  })
}

# Execute if we have coordination results
if (exists("coordination_results") && 
    !is.null(coordination_results$updated_list) && 
    !is.null(coordination_results$new_accounts)) {
  
  log_info("Starting to update account logs")
  update_success <- update_account_logs(coordination_results)
  
  if (update_success) {
    log_info("Successfully completed account log updates")
  } else {
    log_warn("Failed to update account logs")
  }
  
} else {
  log_warn("No coordination results available for logging")
}

# Save global error list if it contains any errors
if (length(global_error_list) > 0) {
  error_log_path <- file.path(daily_output_dir, "error_log.csv")
  
  # Create error data frame with proper error handling
  error_df <- tryCatch({
    do.call(rbind, lapply(global_error_list, function(x) {
      # Ensure all required fields exist with defaults
      data.frame(
        timestamp = as.Date(Sys.time()),
        type = if(!is.null(x$type)) x$type else "unknown",
        message = if(!is.null(x$message)) x$message else "no message",
        stringsAsFactors = FALSE
      )
    }))
  }, error = function(e) {
    # If there's an error creating the data frame, create an empty one with correct structure
    data.frame(
      timestamp = as.Date(character()),
      type = character(),
      message = character(),
      stringsAsFactors = FALSE
    )
  })
  
  # Only write if we have data
  if (nrow(error_df) > 0) {
    write_csv(error_df, error_log_path)
    log_info("Saved error log to file")
  } else {
    log_warn("No valid errors to log")
  }
}

```

```{r load_historical_data, include=TRUE, echo=FALSE}
# Function to load and combine historical data
load_historical_data <- function() {
  # Get all daily directories
  daily_dirs <- list.dirs(file.path(params$base_dir, "output", "daily"), recursive = FALSE)
  
  # Initialize empty lists for different types of data
  historical_data <- list(
    accounts = data.frame(),
    logs = data.frame(),
    summaries = list()
  )
  
  # Process each daily directory
  for (dir in daily_dirs) {
    date <- basename(dir)
    
    # Load account data
    account_file <- file.path(dir, "tiktok_coordinated_accounts_ids.csv")
    if (file.exists(account_file)) {
      accounts <- read_csv(account_file, show_col_types = FALSE) %>%
        mutate(date = as.Date(date))
      historical_data$accounts <- bind_rows(historical_data$accounts, accounts)
    }
    
    # Load log data
    log_file <- file.path(dir, "new_accounts_log.csv")
    if (file.exists(log_file)) {
      logs <- read_csv(log_file, show_col_types = FALSE)
      historical_data$logs <- bind_rows(historical_data$logs, logs)
    }
    
    # Load summary
    summary_file <- file.path(dir, "summary", "execution_summary.txt")
    if (file.exists(summary_file)) {
      historical_data$summaries[[date]] <- readLines(summary_file)
    }
  }
  
  return(historical_data)
}

# Load historical data
historical_data <- load_historical_data()

```

# Today Results

```{r results_summary, include=TRUE, echo=FALSE, results='asis'}

# Function to generate detailed summary
generate_summary <- function(
  coord_users,
  startdate,
  enddate,
  recent_videos,
  all_videos,
  coordination_results,
  global_error_list
) {
  # Initialize empty list for summary parts
  summary_parts <- list()
  
  # Basic collection summary
  summary_parts$collection <- sprintf(
    "We attempted to retrieve videos from **%s monitored accounts**, during the period from %s to %s.",
    scales::comma(length(coord_users)),
    format(as.Date(startdate), "%B %d, %Y"),
    format(as.Date(enddate), "%B %d, %Y")
  )
  
  # Recent videos summary
  if (exists("recent_videos") && is.data.frame(recent_videos) && nrow(recent_videos) > 0) {
    summary_parts$recent <- sprintf(
      "We successfully retrieved **%s recent videos**.",
      scales::comma(nrow(recent_videos))
    )
  } else {
    summary_parts$recent <- "We were unable to retrieve any recent videos due to API issues or no new videos from monitored accounts."
  }
  
  # All videos summary
  if (exists("all_videos") && is.data.frame(all_videos) && nrow(all_videos) > 0) {
    summary_parts$all <- sprintf(
      "Using available data, we accessed a total of **%s videos** posted on TikTok within the timeframe.",
      scales::comma(nrow(all_videos))
    )
  } else {
    summary_parts$all <- "No videos were available for analysis after the full fetch process (possibly due to API quota or no matching descriptions)."
  }
  
  # Coordination analysis summary
  if (exists("coordination_results") && 
      !is.null(coordination_results) && 
      !is.null(coordination_results$summary_accounts) && 
      is.data.frame(coordination_results$summary_accounts) && # check it's a dataframe
      nrow(coordination_results$summary_accounts) > 0 && # check it has rows
      !is.null(coordination_results$coord_graph) &&
      igraph::is_igraph(coordination_results$coord_graph)) { # check it's an igraph object
    
    summary_parts$coordination <- sprintf(
      "Our analysis for coordinated detection in these videos identified **%s accounts** spread across **%s components**, and it also uncovered **%s new accounts** exhibiting coordinated behavior.",
      scales::comma(nrow(coordination_results$summary_accounts)),
      scales::comma(igraph::components(coordination_results$coord_graph)$no),
      scales::comma(if(!is.null(coordination_results$new_accounts)) nrow(coordination_results$new_accounts) else 0)
    )
  } else {
    summary_parts$coordination <- "We were unable to perform coordinated detection analysis due to insufficient data or no coordination detected."
  }
  
  # Generate error summary if there are errors
  error_summary <- generate_error_summary(global_error_list)
  
  # Combine all parts
  full_summary <- paste(unlist(summary_parts), collapse = " ")
  
  # Add error summary if there are errors
  if (nchar(error_summary) > 0) {
    full_summary <- paste(full_summary, "\n\n", error_summary)
  }
  
  # Create output directory if it doesn't exist
  dir.create(file.path(params$base_dir, "output", "summary"), showWarnings = FALSE, recursive = TRUE)
  
  # Save summary to file
  summary_file_path <- file.path(params$base_dir, "output", "summary", "execution_summary.txt")
  writeLines(c(full_summary), summary_file_path)
  
  # Log summary creation
  log_info("Generated execution summary")
  log_info(sprintf("Summary saved to %s", summary_file_path))
  
  return(list(
    summary = full_summary,
    error_summary = error_summary
  ))
}

# Function to generate error summary
generate_error_summary <- function(global_error_list) {
  # Only generate error summary if there are errors
  if (length(global_error_list) > 0) {
    # Create warning box with proper newlines and spacing
    warning_box <- sprintf(
      "::: {.callout-warning}\n\nWarning: %d errors were encountered during execution. Results may be incomplete. Please check the error summary below for details.\n\n:::\n\n",
      length(global_error_list)
    )
    
    # Rest of the code remains the same
    error_types <- table(sapply(global_error_list, function(x) x$type))
    
    error_parts <- c(
      '<details>',
      '<summary>Error Details</summary>',
      '\n',
      sprintf("**Total errors encountered:** %d\n", length(global_error_list)),
      "### Errors by type:\n",
      paste(capture.output(print(error_types)), collapse="\n"),
      "\n### Sample Errors:\n"
    )
    
    sample_size <- min(5, length(global_error_list))
    sample_errors <- sample(global_error_list, sample_size)
    error_samples <- sapply(sample_errors, function(error) {
      sprintf("- **Type:** %s, **Message:** %s", error$type, error$message)
    })
    
    error_details <- c(error_parts, error_samples, '</details>')
    
    return(paste(c(warning_box, error_details), collapse = "\n"))
  }
  
  # Return empty string if no errors
  return("")
}

# Execute summary generation
if (exists("coord_users") && exists("startdate") && exists("enddate")) {
  log_info("Generating execution summary")
  
  summary_result <- generate_summary(
    coord_users = coord_users,
    startdate = startdate,
    enddate = enddate,
    recent_videos = if(exists("recent_videos")) recent_videos else NULL,
    all_videos = if(exists("all_videos")) all_videos else NULL,
    coordination_results = if(exists("coordination_results")) coordination_results else NULL,
    global_error_list = global_error_list
  )
  
  # Print summary to console
  cat("\nExecution Summary:\n")
  cat(summary_result$summary)
  cat("\n")
} else {
  log_warn("Missing required variables for summary generation")
}

```

```{r historical_trends, include=TRUE, echo=FALSE, fig.cap="Historical trends of coordinated accounts"}

# Plot historical trends
if (exists("historical_data") && !is.null(historical_data$logs) && nrow(historical_data$logs) > 0) {
  ggplot(historical_data$logs, aes(x = Timestamp)) +
    geom_line(aes(y = Total_Accounts, color = "Total Accounts")) +
    geom_line(aes(y = New_Accounts_Count, color = "New Accounts")) +
    labs(
      title = "Historical Trends of Coordinated Accounts",
      x = "Date",
      y = "Number of Accounts",
      color = "Metric"
    ) +
    theme_minimal() +
    scale_color_manual(values = c("Total Accounts" = "blue", "New Accounts" = "red"))
} else {
  cat("No historical log data available to plot trends.")
}

```

## Interactive Graph Visualization

```{r generate_network_function, include=FALSE, message=FALSE}

generate_network_visualization <- function(coordination_results,
                                        height = "800px",
                                        seed = 123,
                                        node_size_multiplier = 3,
                                        node_size_base = 10,
                                        edge_width_multiplier = 2) {
  
  if (!requireNamespace("htmltools", quietly = TRUE)) {
    stop("Package 'htmltools' is required")
  }
  
  tryCatch({
    if (is.null(coordination_results) || is.null(coordination_results$coord_graph)) {
      warning("No valid coordination graph available for visualization")
      return(NULL)
    }
    
    # Create output directory
    viz_dir <- file.path(daily_output_dir, "network")
    dir.create(viz_dir, showWarnings = FALSE, recursive = TRUE)
    
    # Get the graph
    graph <- coordination_results$coord_graph
    if (!igraph::is_igraph(graph)) {
      stop("Invalid graph object provided")
    }
    
    # Convert to visNetwork format
    data <- visNetwork::toVisNetworkData(graph)
    
    # Node properties
    data$nodes$label <- data$nodes$id
    data$nodes$value <- igraph::degree(graph)
    data$nodes$title <- sprintf(
      "<div class='network-tooltip'><strong>@%s</strong><br>Connections: %d</div>",
      data$nodes$id,
      data$nodes$value
    )
    data$nodes$shadow <- TRUE
    data$nodes$shape <- "dot"
    data$nodes$url <- paste0("https://tiktok.com/@", data$nodes$id)
    
    # Calculate component membership
    comp <- igraph::components(graph)$membership
    data$nodes$component <- comp
    
    # Add cluster labels if available
    if(exists("cluster_labels") && is.data.frame(get("cluster_labels")) && nrow(get("cluster_labels")) > 0) {
      label_lookup <- get("cluster_labels") %>%
        dplyr::mutate(
          component_id = as.numeric(component),
          group_label = sprintf("Network %d - %s", component_id, label)
        ) %>%
        dplyr::select(component_id, group_label)
      
      data$nodes <- data$nodes %>%
        dplyr::left_join(label_lookup, by = c("component" = "component_id")) %>%
        dplyr::mutate(
          size = value * node_size_multiplier + node_size_base,
          shape = "dot",
          shadow = TRUE,
          group = dplyr::coalesce(group_label, sprintf("Network %d", component))
        )
    } else {
      data$nodes$group <- sprintf("Network %d", comp)
    }
    
    # Edge properties
    data$edges <- data$edges %>%
      dplyr::mutate(
        width = if("weight" %in% names(.)) weight * edge_width_multiplier else 1,
        color = "#94A3B8",
        physics = TRUE
      )
    
    # Network visualization
    network <- visNetwork::visNetwork(
      nodes = data$nodes,
      edges = data$edges,
      width = "100%",
      height = height
    ) %>%
      visNetwork::visLayout(randomSeed = seed) %>%
      visNetwork::visOptions(
        highlightNearest = list(
          enabled = TRUE,
          degree = 1,
          hover = TRUE
        ),
        selectedBy = list(
          variable = "group",
          main = "Select Network"
        ),
        nodesIdSelection = FALSE
      ) %>%
      visNetwork::visPhysics(
        stabilization = list(
          enabled = TRUE,
          iterations = 100
        )
      ) %>%
      visNetwork::visNodes(
        color = list(
          background = "var(--vera-green)",
          border = "var(--vera-dark)",
          highlight = list(
            background = "var(--vera-green)",
            border = "var(--vera-dark)"
          ),
          hover = list(
            background = "var(--vera-green)",
            border = "var(--vera-dark)"
          )
        ),
        font = list(
          color = "var(--vera-dark)"
        ),
        shadow = TRUE
      ) %>%
      visNetwork::visEdges(
        color = list(
          color = "#94A3B8",
          highlight = "var(--vera-green)",
          hover = "var(--vera-green)"
        ),
        smooth = list(enabled = TRUE, type = "dynamic")
      ) %>%
      visNetwork::visInteraction(
        hover = TRUE,
        navigationButtons = TRUE,
        keyboard = TRUE
      ) %>%
      visNetwork::visEvents(
        click = "function(params) {
          if (params.nodes.length > 0) {
            var nodeId = params.nodes[0];
            var nodeData = this.body.data.nodes.get(nodeId);
            if (nodeData.url) {
              window.open(nodeData.url, '_blank');
            }
          }
        }"
      )
    
    # Save visualization
    html_file <- file.path(viz_dir, "coordination_network.html")
    visNetwork::visSave(network, file = html_file, background = "white")
    
    message(sprintf(
      "Network visualization complete. Network has %d nodes and %d edges",
      igraph::gorder(graph),
      igraph::gsize(graph)
    ))
    
    return(network)
    
  }, error = function(e) {
    warning(sprintf("Error generating network visualization: %s", conditionMessage(e)))
    return(NULL)
  })
}

```

```{r generate_visualization, include=FALSE, message=FALSE}
# Generate the visualization and store it in the global environment
network_viz <- local({
  tryCatch({
    if (exists("coordination_results") && !is.null(coordination_results$coord_graph)) {
      generate_network_visualization(coordination_results,                                  
                                     node_size_multiplier = 1.1,
                                     node_size_base = 5,
                                     edge_width_multiplier = 1.1)
    } else {
      warning("No coordination results available for network visualization")
      NULL
    }
  }, error = function(e) {
    warning(paste("Failed to generate network visualization:", e$message))
    NULL
  })
})

# Save the network_viz object to make it available across chunks
assign("network_viz", network_viz, envir = .GlobalEnv)

```

```{r display_visualization, include=TRUE, echo=FALSE}
#| label: fig-network
#| fig-cap: "Interactive visualization of coordinated accounts detected today. Click on a node to visit the respective TikTok account."

if (!is.null(network_viz)) {
  network_viz
} else {
  cat("No network visualization available for today's analysis.")
}

```

## Detailed Tables

```{r setup_tables, include=FALSE, eval=TRUE}

# Helper function to create TikTok profile URL
create_url_link <- function(username, type = "profile") {
  if (type == "profile") {
    sprintf('<a href="https://tiktok.com/@%s" target="_blank">@%s</a>', username, username)
  } else if (type == "video") {
    sprintf('<a href="%s" target="_blank" class="btn btn-sm" style="background-color: #00926c; color: white; text-decoration: none; padding: 2px 8px; border-radius: 4px; font-size: 0.8em;">View Post</a>', username)
  }
}

# Create base datatable with common options
create_datatable_base <- function(data, pageLength = 10, fontSize = "9px", 
                                orderCol = NULL, scrollX = TRUE) {
  options <- list(
    pageLength = pageLength,
    scrollX = scrollX,
    initComplete = JS(sprintf(
      "function(settings, json) { $(this.api().table().container()).css({'font-size': '%s'}); }",
      fontSize
    ))
  )
  
  if (!is.null(orderCol)) {
    options$order <- list(list(orderCol - 1, 'desc'))
  }
  
  datatable(data,
    options = options,
    escape = FALSE,
    rownames = FALSE
  )
}

# 1. Coordinated Posts Table
create_posts_table <- function(summary_groups, all_videos, coord_graph) {
  empty_df_structure <- data.frame(
      `Post URL` = character(), Network = integer(), Author = character(),
      Timestamp = as.POSIXct(character()), `Number of Accounts` = integer(),
      Views = integer(), Likes = integer(), Comments = integer(), Shares = integer(),
      Description = character(), stringsAsFactors = FALSE
    )

  if (is.null(coord_graph) || !igraph::is_igraph(coord_graph) || igraph::gorder(coord_graph) == 0 ||
      is.null(summary_groups) || nrow(summary_groups) == 0 ||
      is.null(all_videos) || nrow(all_videos) == 0) {
    log_warn("Insufficient data for Coordinated Posts Table. Returning empty table.")
    return(create_datatable_base(empty_df_structure[0,], orderCol = 4))
  }

  # Get the list of coordinated accounts from the graph
  coordinated_accounts <- V(coord_graph)$name
  
  # Get component membership for coordinated accounts
  comp <- components(coord_graph)
  account_components <- data.frame(
    author_name = coordinated_accounts,
    network = comp$membership,
    stringsAsFactors = FALSE
  )
  
  # Create posts table, filtering for coordinated accounts only
  posts_df <- summary_groups %>%
    left_join(
      select(all_videos, 
        video_description, video_url, author_name, create_time,
        view_count, like_count, comment_count, share_count
      ),
      by = c("object_id" = "video_description")
    ) %>%
    # Only keep posts from coordinated accounts
    filter(author_name %in% coordinated_accounts) %>%
    # Join with component information
    left_join(account_components, by = "author_name") %>%
    mutate(
      video_link = map_chr(video_url, ~create_url_link(.x, "video")),
      Description = paste0(substr(object_id, 1, 150), "...")
    ) %>%
    select(
      `Post URL` = video_link,
      Network = network,
      Author = author_name,
      Timestamp = create_time,
      `Number of Accounts` = num_accounts,
      Views = view_count,
      Likes = like_count,
      Comments = comment_count,
      Shares = share_count,
      Description = object_id # Keep full description for data, display shortened
    ) %>%
    distinct()

  return(create_datatable_base(posts_df, orderCol = 4))
}

# 2. Coordinated Accounts Table
create_accounts_table <- function(summary_accounts, coord_graph, all_videos) {
  empty_df_structure <- data.frame(
      Account = character(), Network = integer(), `Unique Shares` = integer(),
      `Avg Time Between Shares (sec)` = numeric(), `Edge Symmetry Score` = numeric(),
      Degree = integer(), Strength = numeric(), Betweenness = numeric(),
      `Avg Views` = numeric(), `Avg Likes` = numeric(),
      `Avg Comments` = numeric(), `Avg Shares` = numeric(),
      stringsAsFactors = FALSE
    )

  if (is.null(summary_accounts) || nrow(summary_accounts) == 0 || 
      is.null(coord_graph) || !igraph::is_igraph(coord_graph) || igraph::gorder(coord_graph) == 0) {
    log_warn("Insufficient data for Coordinated Accounts Table. Returning empty table.")
    return(create_datatable_base(empty_df_structure[0,], pageLength = 25, orderCol = 6))
  }

  # Get network metrics
  network_metrics <- list(
    degree = igraph::degree(coord_graph),
    strength = igraph::strength(coord_graph),
    betweenness = igraph::betweenness(coord_graph, normalized = TRUE)
  )
  
  metrics_df <- data.frame(
    account_id = names(network_metrics$degree),
    degree = as.numeric(network_metrics$degree),
    strength = as.numeric(network_metrics$strength),
    betweenness = as.numeric(network_metrics$betweenness),
    stringsAsFactors = FALSE 
  )
  
  # Get engagement metrics
  engagement_metrics <- data.frame(author_name=character(), avg_views=numeric(), avg_likes=numeric(), avg_comments=numeric(), avg_shares=numeric(), stringsAsFactors = FALSE)
  if (!is.null(all_videos) && nrow(all_videos) > 0 && 
      all(c("author_name", "view_count", "like_count", "comment_count", "share_count") %in% colnames(all_videos))) {
      engagement_metrics <- all_videos %>%
        group_by(author_name) %>%
        summarize(
          avg_views = mean(view_count, na.rm = TRUE),
          avg_likes = mean(like_count, na.rm = TRUE),
          avg_comments = mean(comment_count, na.rm = TRUE),
          avg_shares = mean(share_count, na.rm = TRUE),
          .groups = 'drop'
        )
  } else {
      log_warn("all_videos is empty or missing columns for engagement metrics in create_accounts_table.")
  }
  
  accounts_df <- summary_accounts %>%
    mutate(
      account_link = map_chr(account_id, ~create_url_link(.x, "profile")),
      network = igraph::components(coord_graph)$membership[match(account_id, V(coord_graph)$name)]
    ) %>%
    left_join(metrics_df, by = "account_id") %>%
    left_join(engagement_metrics, by = c("account_id" = "author_name")) %>% 
    mutate(across(where(is.numeric), ~ifelse(is.na(.x), 0, round(.x, 3)))) %>% 
    select(
      Account = account_link,
      Network = network,
      `Unique Shares` = unique_shares_count,
      `Avg Time Between Shares (sec)` = avg_time_delta,
      `Edge Symmetry Score` = avg_edge_symmetry_score,
      Degree = degree,
      Strength = strength,
      Betweenness = betweenness,
      `Avg Views` = avg_views,
      `Avg Likes` = avg_likes,
      `Avg Comments` = avg_comments,
      `Avg Shares` = avg_shares
    )
  
  return(create_datatable_base(accounts_df, pageLength = 25, orderCol = 6))
}

# 3. Coordinated Clusters Table
create_clusters_table <- function(coord_graph, summary_accounts, all_videos) {
   empty_df_structure <- data.frame(
      Network = integer(), `Number of Accounts` = integer(),
      `Network Density` = numeric(), `Network Diameter` = numeric(),
      `Clustering Coefficient` = numeric(), `Avg Views` = numeric(),
      `Avg Likes` = numeric(), `Avg Comments` = numeric(), `Avg Shares` = numeric(),
      `Member Accounts` = character(), stringsAsFactors = FALSE
    )
    
  if (is.null(coord_graph) || !igraph::is_igraph(coord_graph) || igraph::gorder(coord_graph) == 0) {
    log_warn("Insufficient data for Coordinated Clusters Table. Returning empty table.")
    return(create_datatable_base(empty_df_structure[0,], fontSize = "12px"))
  }

  comp <- components(coord_graph)
  
  # Calculate cluster metrics
  cluster_metrics <- map_dfr(1:comp$no, function(i) {
    subg <- induced_subgraph(coord_graph, which(comp$membership == i))
    tibble(
      network = i,
      size = sum(comp$membership == i),
      density = edge_density(subg),
      diameter = if(vcount(subg) < 2) 0 else diameter(subg),
      transitivity = transitivity(subg, type = "global"),
      accounts = paste(V(coord_graph)$name[comp$membership == i], collapse = ", ")
    )
  })
  
  # Get engagement metrics by component
  engagement_metrics <- data.frame(network = integer(), avg_views=numeric(), avg_likes=numeric(), avg_comments=numeric(), avg_shares=numeric(), stringsAsFactors = FALSE)
  if (!is.null(all_videos) && nrow(all_videos) > 0 && 
      all(c("author_name", "view_count", "like_count", "comment_count", "share_count") %in% colnames(all_videos))) {
    engagement_metrics <- all_videos %>%
      mutate(
        network = comp$membership[match(author_name, V(coord_graph)$name)]
      ) %>%
      filter(!is.na(network)) %>%
      group_by(network) %>%
      summarize(
        avg_views = mean(view_count, na.rm = TRUE),
        avg_likes = mean(like_count, na.rm = TRUE),
        avg_comments = mean(comment_count, na.rm = TRUE),
        avg_shares = mean(share_count, na.rm = TRUE),
        .groups = 'drop'
      )
  } else {
    log_warn("all_videos is empty or missing columns for engagement metrics in create_clusters_table.")
  }
  
  clusters_df <- cluster_metrics %>%
    left_join(engagement_metrics, by = "network") %>%
    mutate(across(where(is.numeric), ~ifelse(is.na(.x), 0, round(.x, 3)))) %>%
    select(
      Network = network,
      `Number of Accounts` = size,
      `Network Density` = density,
      `Network Diameter` = diameter,
      `Clustering Coefficient` = transitivity,
      `Avg Views` = avg_views,
      `Avg Likes` = avg_likes,
      `Avg Comments` = avg_comments,
      `Avg Shares` = avg_shares,
      `Member Accounts` = accounts
    )
  
  return(create_datatable_base(clusters_df, fontSize = "12px") %>%
    formatStyle(
      columns = 2:9,
      className = 'dt-right'
    ) %>%
    formatStyle(
      'Network',
      width = '300px'
    ))
}

# Create all tables
if (exists("coordination_results") && !is.null(coordination_results) &&
    exists("all_videos") && !is.null(all_videos)) {
  
  accounts_table <- create_accounts_table(
    coordination_results$summary_accounts, 
    coordination_results$coord_graph,
    all_videos
  )
  
  clusters_table <- create_clusters_table(
    coordination_results$coord_graph, 
    coordination_results$summary_accounts, # Though not directly used, implies coord_graph validity
    all_videos
  )
  
  posts_table <- create_posts_table(
    coordination_results$summary_groups, 
    all_videos,
    coordination_results$coord_graph
  )
} else {
  log_warn("coordination_results or all_videos not available. Tables will be empty.")
}

# Fallback initialization if tables were not created
empty_df_msg <- data.frame(Message = "No data available for this table.")
if (!exists("accounts_table")) { 
  accounts_table <- create_datatable_base(empty_df_msg, pageLength = 25, orderCol = NULL)
}
if (!exists("clusters_table")) {
  clusters_table <- create_datatable_base(empty_df_msg, fontSize = "12px", orderCol = NULL)
}
if (!exists("posts_table")) {
  posts_table <- create_datatable_base(empty_df_msg, orderCol = NULL)
}

```

```{r display_tables, echo=FALSE, results='asis'}
# Display tables with headers
cat("\n")  # Ensure proper spacing
```

### Coordinated Accounts
```{r display_accounts, echo=FALSE}
if (exists("accounts_table")) {
  accounts_table
} else {
  cat("Accounts table could not be generated.")
}
```

### Coordinated Networks/Clusters
```{r display_clusters, echo=FALSE}
if (exists("clusters_table")) {
  clusters_table
} else {
  cat("Clusters table could not be generated.")
}
```

### Coordinated Posts
```{r display_posts, echo=FALSE}
if (exists("posts_table")) {
  posts_table
} else {
  cat("Posts table could not be generated.")
}
```

```{r download-section, echo=FALSE}

# Function to prepare download files 
prepare_download_files <- function() {
  temp_dir <- tempdir()
  current_wd <- getwd()
  
  # Change to temp dir for flat zip structure
  setwd(temp_dir)
  
  # Files to zip
  files_to_zip <- c()
  
  # Save tables as CSV if they exist and are datatable objects with data
  if (exists("accounts_table") && inherits(accounts_table, "datatables") && nrow(accounts_table$x$data) > 0) {
    write.csv(accounts_table$x$data, "coordinated_accounts.csv", row.names = FALSE)
    files_to_zip <- c(files_to_zip, "coordinated_accounts.csv")
  }
  if (exists("clusters_table") && inherits(clusters_table, "datatables") && nrow(clusters_table$x$data) > 0) {
    write.csv(clusters_table$x$data, "network_clusters.csv", row.names = FALSE)
    files_to_zip <- c(files_to_zip, "network_clusters.csv")
  }
  if (exists("posts_table") && inherits(posts_table, "datatables") && nrow(posts_table$x$data) > 0) {
    write.csv(posts_table$x$data, "coordinated_posts.csv", row.names = FALSE)
    files_to_zip <- c(files_to_zip, "coordinated_posts.csv")
  }
  
  # Export network graph if it exists
  if (exists("coordination_results") && !is.null(coordination_results$coord_graph)) {
    igraph::write_graph(coordination_results$coord_graph, "coordination_network.graphml", format = "graphml")
    files_to_zip <- c(files_to_zip, "coordination_network.graphml")
  }
  
  # Create README with dynamic parameters
  readme_template <- '# TikTok Coordination Analysis Results Guide

This package contains the results of coordinated behavior analysis on TikTok performed on %s. 
For the latest daily analysis, visit: https://fabiogiglietto.github.io/tiktok_csbn/tt_viz.html

Below you\'ll find detailed explanations of the data fields in each table.

## Data Files

The package includes the following files:
- `coordinated_posts.csv`: Data about posts that show coordinated sharing patterns
- `coordinated_accounts.csv`: Information about accounts involved in coordination
- `network_clusters.csv`: Analysis of the identified coordinated networks
- `coordination_network.graphml`: Network graph file that can be opened with tools like Gephi

## Table Fields Description

### coordinated_posts.csv
| Field | Description |
|-------|-------------|
| Post URL | Direct link to the TikTok post |
| Network | Numerical identifier of the coordinated network the post author belongs to |
| Author | Username of the TikTok account that shared the post |
| Timestamp | Date and time when the post was created |
| Number of Accounts | Number of accounts that shared identical or near-identical content |
| Views | Total number of views the post received |
| Likes | Total number of likes the post received |
| Comments | Total number of comments on the post |
| Shares | Total number of times the post was shared |
| Description | First 150 characters of the post\'s text description |

### coordinated_accounts.csv
| Field | Description |
|-------|-------------|
| Account | Username of the TikTok account |
| Network | Numerical identifier of the coordinated network the account belongs to |
| Unique Shares | Number of unique pieces of content shared by this account that were also shared by other accounts |
| Avg Time Between Shares (sec) | Average time difference between when this account and other accounts shared the same content |
| Edge Symmetry Score | Measure of reciprocity in coordinated behavior (0-1, where 1 indicates perfect symmetry) |
| Degree | Number of direct connections to other accounts in the coordination network |
| Strength | Weighted measure of connections, accounting for frequency of coordination |
| Betweenness | Measure of the account\'s role in bridging different parts of the network (normalized 0-1) |
| Avg Views | Average number of views per post |
| Avg Likes | Average number of likes per post |
| Avg Comments | Average number of comments per post |
| Avg Shares | Average number of shares per post |

### network_clusters.csv
| Field | Description |
|-------|-------------|
| Network | Numerical identifier of the coordinated network |
| Number of Accounts | Total number of accounts in this network |
| Network Density | Proportion of potential connections that are actual connections (0-1) |
| Network Diameter | Maximum number of steps needed to connect any two accounts in the network |
| Clustering Coefficient | Measure of how tightly clustered the accounts are (0-1) |
| Avg Views | Average number of views per post across all accounts in the network |
| Avg Likes | Average number of likes per post across all accounts in the network |
| Avg Comments | Average number of comments per post across all accounts in the network |
| Avg Shares | Average number of shares per post across all accounts in the network |
| Member Accounts | Comma-separated list of all usernames in this network |

## Analysis Parameters

The coordination detection was performed with the following parameters:
- Time Window: %d seconds
- Minimum Participation: %d posts
- Edge Weight Threshold: %.1f
- Days Analyzed: %d

## Notes

- All timestamps are in UTC
- Network IDs are consistent across all files
- The betweenness centrality is normalized to allow comparison between networks of different sizes
- View, like, comment, and share counts are captured at the time of data collection and may not reflect current values

## References

For more information about the methodology and tools used, please refer to:

Righetti N., Balluff P. (2023). CooRTweet: Coordinated Networks Detection on Social Media. R package version 2.1.0. https://CRAN.R-project.org/package=CooRTweet

Giglietto, F., Marino, G., Mincigrucci, R., & Stanziano, A. (2023). A Workflow to Detect, Monitor, and Update Lists of Coordinated Social Media Accounts Across Time: The Case of the 2022 Italian Election. Social Media + Society, 9(3).'

  # Insert actual parameter values into the template
  # Format the current date
  current_date <- format(Sys.Date(), "%B %d, %Y")
  
  # Insert all parameters into the template
  readme_content <- sprintf(readme_template, 
                          current_date,
                          params$time_window, 
                          params$min_participation, 
                          params$edge_weight,
                          params$days_back)
  
  # Save README file
  writeLines(readme_content, "README.md")
  files_to_zip <- c(files_to_zip, "README.md")
  
  # Create zip file with date
  current_date_fmt <- format(Sys.Date(), "%Y-%m-%d") # Renamed for clarity
  daily_zip_name <- sprintf("coordination_results_%s.zip", current_date_fmt)
  zip_file <- file.path(daily_output_dir, daily_zip_name)
  
  # Only include files that actually exist
  existing_files <- files_to_zip[file.exists(files_to_zip)]
  if (length(existing_files) > 0) {
    utils::zip(zip_file, files = existing_files)
  }
  
  # Restore working directory
  setwd(current_wd)
  return(list(
    file = if (file.exists(zip_file)) zip_file else NULL,
    filename = daily_zip_name
  ))
}

# Create the zip file
zip_result <- prepare_download_files()

# Create download button HTML
if (!is.null(zip_result$file)) {
  download_url <- file.path("daily", format(Sys.Date(), "%Y-%m-%d"), zip_result$filename)
  download_html <- sprintf('
    <div style="background-color: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0;">
      <h3>Download Results</h3>
      <p>Download a comprehensive package of today\'s coordination analysis results, including:</p>
      <ul style="margin-bottom: 15px;">
        <li>Coordinated accounts table (CSV format)</li>
        <li>Network clusters analysis (CSV format)</li>
        <li>Coordinated posts data (CSV format)</li>
        <li>Network graph (GraphML format)</li>
        <li>README file with detailed field descriptions</li>
      </ul>
      <a href="%s" download class="btn btn-primary" style="display: inline-block; padding: 8px 16px; background-color: #00926c; color: white; text-decoration: none; border-radius: 4px; border: none; font-family: \'Comfortaa\', sans-serif;">
        Download Results Package
      </a>
    </div>
  ', download_url)
  
  htmltools::HTML(download_html)
}

```

# About

|                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                           |
|--------------------|----------------------------------------------------|
| [![vera ai logo](https://www.disinfo.eu/wp-content/uploads/elementor/thumbs/vera-logo_black-pz7er90kthmarde380cigj2nwx09ubmujp4y24avw2.jpg)](https://www.veraai.eu/) | [vera.ai](https://www.veraai.eu/home) is a research and development project focusing on disinformation analysis and AI supported verification tools and services. Project funded by EU Horizon Europe, the UK's innovation agency, and the Swiss State Secretariat for Education, Research and Innovation |

# References

Giglietto, F., Marino, G., Mincigrucci, R., & Stanziano, A. (2023). A Workflow to Detect, Monitor, and Update Lists of Coordinated Social Media Accounts Across Time: The Case of the 2022 Italian Election. Social Media + Society, 9(3). https://doi.org/10.1177/20563051231196866

# Session Info

```{r session_info, include=TRUE, echo=FALSE}
sessionInfo()
```