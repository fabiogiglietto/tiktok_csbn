---
title: "TikTok Coordinated Detection Project"
author: "Fabio Giglietto"
date: "`r Sys.Date()`"
format: 
  html:
    code-fold: true
    toc: true
    number-sections: true
    include-in-header:
      - text: |
          <style>
          /* Vera.ai Brand Styles */
          @import url('https://fonts.googleapis.com/css2?family=Comfortaa:wght@300;700&display=swap');
          
          :root {
            --vera-green: #00926c;
            --vera-dark: #3d3d3c;
            --vera-white: #ffffff;
          }
          
          body {
            font-family: 'Comfortaa', sans-serif;
            color: var(--vera-dark);
          }
          
          /* Table Styles */
          .dataTables_wrapper {
            font-family: 'Comfortaa', sans-serif !important;
          }
          
          .dataTables_wrapper .dataTables_filter input {
            border: 1px solid var(--vera-green);
            border-radius: 4px;
            padding: 4px 8px;
          }
          
          .dataTables_wrapper .dataTables_length select {
            border: 1px solid var(--vera-green);
            border-radius: 4px;
          }
          
          .dataTables_wrapper .dataTables_paginate .paginate_button.current {
            background: var(--vera-green) !important;
            color: var(--vera-white) !important;
            border: none;
            border-radius: 4px;
          }
          
          .dataTables_wrapper .dataTables_paginate .paginate_button:hover {
            background: var(--vera-dark) !important;
            color: var(--vera-white) !important;
            border: none;
          }
          
          /* Chart Styles */
          .recharts-default-tooltip {
            background-color: var(--vera-white) !important;
            border: 1px solid var(--vera-green) !important;
            border-radius: 4px;
            font-family: 'Comfortaa', sans-serif;
          }
          
          /* Network Tooltip Styles */
          .network-tooltip {
            font-family: 'Comfortaa', sans-serif;
            background-color: var(--vera-white);
            border: 1px solid var(--vera-green);
            border-radius: 4px;
            padding: 12px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
          }
          
          .network-tooltip a {
            color: var(--vera-green);
            text-decoration: none;
          }
          
          .network-tooltip a:hover {
            text-decoration: underline;
          }
          
          /* Network Visualization Styles */
          .vis-network {
            background-color: var(--vera-white);
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
          }
          
          .vis-network .vis-navigation {
            font-family: 'Comfortaa', sans-serif;
          }
          
          /* Layout Fixes */
          #quarto-content {
            grid-template-rows: none !important;
            grid-auto-rows: min-content !important;
            font-family: 'Comfortaa', sans-serif;
          }
          
          /* Table Cell Wrapping */
          .dt-head-left, 
          .dt-body-left {
            white-space: normal !important;
            color: var(--vera-dark);
            padding: 8px !important;
          }
          
          /* DataTables Cell Links */
          .dt-head-left a, 
          .dt-body-left a {
            color: var(--vera-green);
            text-decoration: none;
          }
          
          .dt-head-left a:hover, 
          .dt-body-left a:hover {
            text-decoration: underline;
          }
          
          /* Card Styles */
          .card {
            background: var(--vera-white);
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            padding: 24px;
            margin-bottom: 24px;
          }
          
          .card-title {
            color: var(--vera-dark);
            font-family: 'Comfortaa', sans-serif;
            font-weight: 700;
            font-size: 1.5rem;
            margin-bottom: 16px;
          }
          
          /* Button Styles */
          .vera-button {
            background-color: var(--vera-green);
            color: var(--vera-white);
            border: none;
            border-radius: 4px;
            padding: 8px 16px;
            font-family: 'Comfortaa', sans-serif;
            cursor: pointer;
            transition: background-color 0.2s;
          }
          
          .vera-button:hover {
            background-color: var(--vera-dark);
          }
          </style>  
params:
  load_from_snapshot: FALSE
  base_dir: "/home/mine/VERAAI_WP4"
  time_window: 180
  min_participation: 2
  edge_weight: 0.5
  days_back: 15
  ai_model: "chatgpt-4o-latest"
---

# Introduction

This document serves as a proof of concept, developed within the framework of the [vera.ai Horizon EU project](https://www.veraai.eu/). It presents a comprehensive methodology for tracking and analyzing coordinated sharing activities on TikTok, employing [`traktok`](https://github.com/JBGruber/traktok) for data collection via the TikTok Research API and [`CooRTweet`](https://github.com/nicolarighetti/CooRTweet) for the analysis of coordinated behavior patterns.

**Initial Discovery:** The analysis initiates by focusing on content tagged with the #moskow hashtag. This first step successfully identifies a preliminary group of accounts involved in coordinated sharing activities in the aftermath of the attack in Moscow. This discovery lays the groundwork for an extensive examination of coordinated dynamics across social media platforms.

On April 19, 2024, we received a list of 513 problematic accounts from a trusted partner within the <i>vera ai</i> consortium. Accounts that were mentioned at least twice on this list have been added to our pool of monitored accounts.

```{r write_description, results='asis', echo=FALSE}
cat(sprintf(
"**Daily Monitoring and Analysis:** Subsequent to the initial identification, the methodology transitions into a phase of daily monitoring. In this phase, the script consistently retrieves videos posted by the previously identified accounts, with the goal of detecting both ongoing and emerging instances of coordinated behavior. As new accounts manifesting coordinated behavior (time_window = %d seconds, min_participation = %d posts, edge_weight = %.1f) are discovered, they are incorporated into the daily monitoring routine. For each identified network of coordinated accounts, AI-generated labels are automatically created using the %s model by analyzing the content patterns, themes, and characteristics of the posts shared within each network, providing a concise description of the common elements that unite these accounts in their coordinated behavior.", 
params$time_window,
params$min_participation,
params$edge_weight,
params$ai_model
))
```

This approach ensures continuous updates on the number of newly discovered coordinated accounts, highlighting the fluid nature of social media coordination. Enhanced by interactive visualizations, the analysis sheds light on the shifting landscape of account activities and the intricate network of interactions among them on the TikTok platform.

By delineating these processes, the proof of concept underscores the potential for advanced analytical tools to reveal and understand the complex phenomena of coordinated social media behavior within the context of significant societal events.

```{r setup, include=FALSE, message=FALSE}

# Set working directory
setwd(params$base_dir)

# Library loading
library(tidyverse)
library(CooRTweet)
library(httr)
library(purrr)
library(scales)
library(stringr)
library(igraph)
library(visNetwork)
library(ggplot2)
library(jsonlite)
library(lubridate)
library(progress)
library(logger)
library(traktok)
library(DT)
library(htmltools)

# Initialize logging
log_info("Starting TikTok coordinated activity detection script")

# Initialize global error list
global_error_list <- list()

# Secure credential retrieval
get_tiktok_credentials <- function() {
  list(
    client_key = Sys.getenv("TIKTOK_CLIENT_KEY"),
    client_secret = Sys.getenv("TIKTOK_CLIENT_SECRET")
  )
}

# Authentication using traktok package
request_access_token <- function() {
  tryCatch({
    token <- traktok::auth_research(
      client_key = Sys.getenv("TIKTOK_CLIENT_KEY"),
      client_secret = Sys.getenv("TIKTOK_CLIENT_SECRET")
    )
    log_info("Successfully obtained access token using traktok")
    return(token$access_token)
  }, error = function(e) {
    log_error(paste("Error in request_access_token using traktok:", e$message))
    stop(e)
  })
}

# Main setup function
setup <- function() {
  credentials <- get_tiktok_credentials()
  
  if (!params$load_from_snapshot) {
    # Ensure environment variables are set
    if (is.na(Sys.getenv("TIKTOK_CLIENT_KEY")) | is.na(Sys.getenv("TIKTOK_CLIENT_SECRET"))) {
      log_error("Environment variables for TikTok API not set")
      stop("Environment variables for TikTok API not set. Please set TIKTOK_CLIENT_KEY and TIKTOK_CLIENT_SECRET.")
    }
    
    tryCatch({
      access_token <- request_access_token()
    }, error = function(e) {
      log_error(paste("Authentication with TikTok API failed:", e$message))
      stop("Authentication with TikTok API failed. Error message: ", e$message)
    })
    
  } else {
    # Load data from the latest snapshot
    all_videos <- readr::read_csv("/home/mine/VERAAI_WP4/output/latest_snapshot.csv",
                                 col_types = cols(video_id = col_character(), 
                                                effect_ids = col_character(),
                                                music_id = col_character(), 
                                                hashtag_names = col_character()))
    recent_videos <- NA
  }
  
  # Try reading the CSV file
  tryCatch({
    account_ids <- readr::read_csv("/home/mine/VERAAI_WP4/lists/tiktok_coordinated_accounts_ids.csv")
  }, error = function(e) {
    log_error(paste("Failed to read TikTok coordinated accounts ID CSV:", e$message))
    stop("Failed to read TikTok coordinated accounts ID CSV. Error: ", e$message)
  })
  
  coord_users <- unique(account_ids$x)
  enddate <- Sys.Date()
  startdate <- enddate - 15
  
  # Create a list of values to return but don't include params
  setup_values <- list(
    coord_users = coord_users,
    enddate = enddate,
    startdate = startdate,
    access_token = if(exists("access_token")) access_token else NULL,
    all_videos = if(exists("all_videos")) all_videos else NULL
  )
  
  # Assign values to global environment individually
  for(name in names(setup_values)) {
    assign(name, setup_values[[name]], envir = .GlobalEnv)
  }
}

# Add new function to create daily directory
create_daily_output_dir <- function() {
  today_date <- format(Sys.Date(), "%Y-%m-%d")
  daily_dir <- file.path(params$base_dir, "output", "daily", today_date)
  dir.create(daily_dir, showWarnings = FALSE, recursive = TRUE)
  
  # Create subdirectories for different types of output
  dir.create(file.path(daily_dir, "coordination"), showWarnings = FALSE)
  dir.create(file.path(daily_dir, "network"), showWarnings = FALSE)
  dir.create(file.path(daily_dir, "summary"), showWarnings = FALSE)
  
  return(daily_dir)
}

# Create daily directory
daily_output_dir <- create_daily_output_dir()

# Run setup directly
setup()

```

```{r fetch_recent_videos, include=FALSE, message=FALSE, eval=!params$load_from_snapshot}

# Function to get videos using traktok package
get_recent_videos <- function(usernames, start_date, end_date, chunk_size = 50) {
  # Split usernames into chunks
  username_chunks <- split(usernames, ceiling(seq_along(usernames) / chunk_size))
  total_chunks <- length(username_chunks)
  
  # Initialize results
  all_results <- data.frame()
  failure_count <- 0
  incomplete_count <- 0
  
  # Create progress bar
  pb <- progress_bar$new(
    format = "[:bar] :percent :etas",
    total = total_chunks,
    clear = FALSE
  )
  
  # Process each chunk
  for (i in seq_along(username_chunks)) {
    log_info(sprintf("Processing chunk %d of %d", i, total_chunks))
    
    # Get current chunk
    chunk <- as.character(unlist(username_chunks[i]))
    
    tryCatch({
      # First try with max_pages = 1 to ensure we get at least the first page
      chunk_result <- query() |>
        query_and(
          field_name = "username",
          operation = "IN",
          field_values = chunk
        ) |>
        tt_search_api(
          start_date = start_date,
          end_date = end_date,
          max_pages = 1
        )
      
      log_info(sprintf("Retrieved first page with %d results for chunk %d", 
                      nrow(chunk_result), i))
      
      # Try to get more pages, but catch pagination errors
      tryCatch({
        full_results <- query() |>
          query_and(
            field_name = "username",
            operation = "IN",
            field_values = chunk
          ) |>
          tt_search_api(
            start_date = start_date,
            end_date = end_date,
            max_pages = 100
          )
        
        # If pagination succeeded, use the full results
        if (is.data.frame(full_results) && nrow(full_results) > 0) {
          chunk_result <- full_results
          log_info(sprintf("Successfully retrieved all pages with %d total results", 
                          nrow(chunk_result)))
        }
      }, error = function(e) {
        if (grepl("400|invalid_params|Search Id.*invalid or expired", e$message)) {
          log_warn("Pagination failed with 400 error - keeping first page results")
          incomplete_count <- incomplete_count + 1
          global_error_list <<- c(global_error_list, list(list(
            type = "pagination_error",
            chunk = i,
            message = e$message
          )))
        } else {
          log_error(sprintf("Unexpected error during pagination: %s", conditionMessage(e)))
        }
      })
      
      # Add results to main dataframe
      if (is.data.frame(chunk_result) && nrow(chunk_result) > 0) {
        all_results <- bind_rows(all_results, chunk_result)
        log_info(sprintf("Added %d videos from chunk %d to final results", 
                        nrow(chunk_result), i))
      } else {
        failure_count <- failure_count + 1
        global_error_list <<- c(global_error_list, list(list(
          type = "empty_chunk",
          chunk = i,
          usernames = chunk
        )))
      }
      
    }, error = function(e) {
      log_error(sprintf("Error in chunk %d: %s", i, conditionMessage(e)))
      log_error(sprintf("Usernames in failed chunk: %s", paste(chunk, collapse = ", ")))
      failure_count <- failure_count + 1
      global_error_list <<- c(global_error_list, list(list(
        type = "chunk_error",
        chunk = i,
        message = e$message,
        usernames = chunk
      )))
    })
    
    # Update progress bar
    pb$tick()
    
    # Add a small delay between chunks
    Sys.sleep(2)
  }
  
  # Final summary
  summary_msg <- sprintf("\nCollection completed:\n- Total videos: %d\n- Failed chunks: %d/%d\n- Incomplete chunks: %d/%d",
                        nrow(all_results), failure_count, total_chunks, incomplete_count, total_chunks)
  log_info(summary_msg)
  
  if (nrow(all_results) > 0) {
    log_info(sprintf("Unique users in results: %d", length(unique(all_results$username))))
    log_info(sprintf("Date range: %s to %s", 
                    min(as.Date(all_results$create_time)), 
                    max(as.Date(all_results$create_time))))
  }
  
  return(list(
    data = all_results,
    summary = list(
      total_videos = nrow(all_results),
      failed_chunks = failure_count,
      incomplete_chunks = incomplete_count,
      total_chunks = total_chunks
    )
  ))
}

# Main fetch function
fetch_recent_videos <- function() {
  if (!params$load_from_snapshot) {
    log_info("Starting video collection")
    recent_videos_result <- get_recent_videos(
      usernames = coord_users,
      start_date = startdate,
      end_date = enddate
    )
    recent_videos <- recent_videos_result$data
    log_info(sprintf("Video collection completed with %d videos", nrow(recent_videos)))
    return(recent_videos)
  } else {
    log_info("Loading from snapshot, skipping video collection")
    return(NULL)
  }
}

# If running directly, execute fetch
if (exists("coord_users")) {
  recent_videos <- fetch_recent_videos()
}


```

```{r fetch_all_videos, include=FALSE, message=FALSE, eval=!params$load_from_snapshot}

# Function to get videos for a specific description
get_videos_by_description <- function(desc, start_date, end_date) {
 log_info(sprintf("Querying for description: %s", desc))
 
 tryCatch({
   # First try with max_pages = 1 to ensure we get at least the first page
   chunk_result <- query() |>
     query_and(
       field_name = "keyword",
       operation = "EQ",
       field_values = list(desc)  # Wrap in list to ensure proper handling
     ) |>
     tt_search_api(
       start_date = start_date,
       end_date = end_date,
       max_pages = 1
     )
   
   log_info(sprintf("Retrieved first page with %d results for description: %s", 
                   nrow(chunk_result), desc))
   
   # Try to get more pages, but catch pagination errors
   tryCatch({
     full_results <- query() |>
       query_and(
         field_name = "keyword",
         operation = "EQ",
         field_values = list(desc)  # Wrap in list here too
       ) |>
       tt_search_api(
         start_date = start_date,
         end_date = end_date,
         max_pages = 100
       )
     
     # If pagination succeeded, use the full results
     if (is.data.frame(full_results) && nrow(full_results) > 0) {
       chunk_result <- full_results
       log_info(sprintf("Successfully retrieved all pages with %d total results", 
                       nrow(chunk_result)))
     }
   }, error = function(e) {
     if (grepl("400|invalid_params|Search Id.*invalid or expired", e$message)) {
       log_warn("Pagination failed with 400 error - keeping first page results")
       global_error_list <<- c(global_error_list, list(list(
         type = "pagination_error",
         description = desc,
         message = e$message
       )))
     } else {
       log_error(sprintf("Unexpected error during pagination: %s", conditionMessage(e)))
     }
   })
   
   return(chunk_result)
   
 }, error = function(e) {
   log_error(sprintf("Error querying description '%s': %s", desc, conditionMessage(e)))
   global_error_list <<- c(global_error_list, list(list(
     type = "description_query_error",
     description = desc,
     message = e$message
   )))
   return(data.frame())
 })
}

# Main function to fetch all videos with matching descriptions
fetch_all_videos <- function(recent_videos, start_date, end_date) {
 
 # Get unique descriptions that are long enough to be meaningful
 unique_descriptions <- unique(recent_videos$video_description[nchar(recent_videos$video_description) >= 80])
 
 log_info(sprintf("Found %d unique descriptions to query", length(unique_descriptions)))
 
 # Initialize progress bar
 pb <- progress_bar$new(
   format = "[:bar] :percent :etas",
   total = length(unique_descriptions),
   clear = FALSE
 )
 
 # Initialize results
 all_results <- data.frame()
 
 # Process each description
 for (desc in unique_descriptions) {
   # Get videos for this description
   videos <- get_videos_by_description(desc, start_date, end_date)
   
   # Add results if any
   if (is.data.frame(videos) && nrow(videos) > 0) {
     all_results <- bind_rows(all_results, videos)
     log_info(sprintf("Added %d videos for description: %s", nrow(videos), substr(desc, 1, 50)))
   }
   
   # Update progress bar
   pb$tick()
   
   # Add delay between requests
   Sys.sleep(2)
 }
 
 # Process the combined data
 if (nrow(all_results) > 0) {
   final_results <- all_results %>%
     mutate(
       region_code = toupper(region_code),
       video_url = paste0("https://www.tiktok.com/@", author_name, "/video/", video_id)
     ) %>%
     filter(!is.na(video_description) & video_description != "") %>%
     distinct()
   
   # Log summary
   log_info(sprintf("\nCollection completed:"))
   log_info(sprintf("Total videos collected: %d", nrow(final_results)))
   log_info(sprintf("Unique users: %d", n_distinct(final_results$author_name)))
   log_info(sprintf("Unique descriptions: %d", n_distinct(final_results$video_description)))
   
   # Create output directory if it doesn't exist
   output_dir <- file.path(params$base_dir, "output")
   dir.create(output_dir, showWarnings = FALSE, recursive = TRUE)
   
   # Save results
   output_file <- file.path(output_dir, "latest_snapshot.csv")
   daily_output_file <- file.path(daily_output_dir, "daily_snapshot.csv")
   write_csv(final_results, output_file)
   write_csv(final_results, daily_output_file)
   log_info(sprintf("Results saved to %s", output_file))
   
   return(final_results)
 } else {
   log_warn("No results found")
   return(data.frame())
 }
}

# Execute if we have recent_videos
if (exists("recent_videos") && is.data.frame(recent_videos) && nrow(recent_videos) > 0) {
 log_info("Starting to fetch all videos with matching descriptions")
 all_videos <- fetch_all_videos(recent_videos, startdate, enddate)
 
 # Assign to global environment
 assign("all_videos", all_videos, envir = .GlobalEnv)
} else {
 log_warn("No recent_videos found or empty dataset")
}

```

```{r coordinated_detection, include=FALSE, message=FALSE}

# Function to detect coordination patterns
detect_coordination <- function(all_videos) {
  tryCatch({
    if(nrow(all_videos) > 0 && "video_description" %in% colnames(all_videos)) {
      # Prepare data with correct field mappings
      prep_data <- CooRTweet::prep_data(
        x = all_videos,
        object_id = "video_description",
        account_id = "author_name",      # Changed from username to author_name
        content_id = "video_id",         # Already correct
        timestamp_share = "create_time"   # Already correct
      )
      
      log_info("Data prepared for coordination detection")
      
      # Detect coordinated groups
      result <- CooRTweet::detect_groups(
        x = prep_data,
        time_window = params$time_window,
        min_participation = params$min_participation,
        remove_loops = TRUE)
      
      coord_graph <- CooRTweet::generate_coordinated_network(
        x = result, 
        edge_weight = params$edge_weight,
        objects = TRUE,
        subgraph = 1)
      
      log_info("Coordination network generated")
      
      # Calculate summary statistics if graph is valid
      if(!is.null(coord_graph) && !is.null(result)) {
        if(!("igraph" %in% class(coord_graph))) {
          log_error("coord_graph is not of type igraph")
          stop("coord_graph is not of type igraph. Please check the generate_coordinated_network output.")
        }
        
        # Calculate summary statistics
        summary_groups <- CooRTweet::group_stats(
          coord_graph = coord_graph, 
          weight_threshold = "full"
        )
        
        summary_accounts <- CooRTweet::account_stats(
          coord_graph = coord_graph, 
          result = result, 
          weight_threshold = "full"
        )
        
        log_info("Successfully calculated summary statistics")
        
        # Get current list of coordinated accounts from summary_accounts
        current_coordinated_accounts <- summary_accounts$account_id
        log_info(sprintf("Found %d accounts showing coordination in current analysis", 
                        length(current_coordinated_accounts)))
        
        # Get the starting list of known accounts
        log_info(sprintf("Starting with %d known coordinated accounts", 
                        length(coord_users)))
        
        # Identify truly new accounts (not in starting list)
        new_account_ids <- data.frame(
          account_id = setdiff(current_coordinated_accounts, coord_users)
        )
        
        # Create updated list
        updated_list <- unique(c(coord_users, new_account_ids$account_id))
        
        # Log the results
        if(nrow(new_account_ids) > 0) {
          log_info(sprintf("Found %d new coordinated accounts (not in starting list)", 
                          nrow(new_account_ids)))
          log_info(sprintf("Total unique accounts after update: %d", 
                          length(updated_list)))
        } else {
          log_info("No new coordinated accounts found")
        }
        
        # Save results
        dir.create("output/coordination", showWarnings = FALSE, recursive = TRUE)
        
        write_csv(summary_groups, "output/coordination/summary_groups.csv")
        write_csv(summary_accounts, "output/coordination/summary_accounts.csv")
        write_csv(data.frame(account_id = updated_list), 
                  "output/coordination/updated_account_list.csv")
        
        return(list(
          coord_graph = coord_graph,
          summary_groups = summary_groups,
          summary_accounts = summary_accounts,
          new_accounts = new_account_ids,
          updated_list = updated_list
        ))
        
      } else {
        log_warn("coord_graph or result is NULL. Skipping summary statistics.")
        return(list(
          new_accounts = data.frame(),
          updated_list = coord_users
        ))
      }
    } else {
      log_warn("No videos available for coordinated detection analysis")
      return(list(
        new_accounts = data.frame(),
        updated_list = coord_users
      ))
    }
  }, error = function(e) {
    log_error(paste("Error in coordinated detection:", e$message))
    global_error_list <<- c(global_error_list, list(list(
      type = "coordinated_detection",
      message = e$message
    )))
    return(list(
      new_accounts = data.frame(),
      updated_list = coord_users
    ))
  })
}

# Execute if we have all_videos
if (exists("all_videos") && is.data.frame(all_videos) && nrow(all_videos) > 0) {
  log_info("Starting coordination detection analysis")
  coordination_results <- detect_coordination(all_videos)
  
  # Update global variables if needed
  if (exists("coordination_results") && !is.null(coordination_results$updated_list)) {
    coord_users <- coordination_results$updated_list
  }
} else {
  log_warn("No videos available for coordination analysis")
}

```

```{r label_clusters, include=FALSE, message=FALSE}

# Function to generate cluster labels using OpenAI API
generate_cluster_labels <- function(coordination_results, all_videos) {
  
  # Check for API credentials
  if (Sys.getenv("OPENAI_VERAAI_API_KEY") == "" || Sys.getenv("OPENAI_VERAAI_ORG_ID") == "") {
    log_warn("OpenAI VERAAI credentials not set")
    return(NULL)
  }
  
  # Function to get cluster content
  get_cluster_content <- function(cluster_id, max_videos = 20) {
    # Get accounts in this cluster
    cluster_accounts <- V(coordination_results$coord_graph)$name[
      components(coordination_results$coord_graph)$membership == cluster_id
    ]
    
    # Get videos from these accounts
    cluster_videos <- all_videos %>%
      filter(author_name %in% cluster_accounts) %>%
      mutate(
        total_engagement = view_count + like_count + share_count + comment_count
      ) %>%
      arrange(desc(total_engagement)) %>%
      head(max_videos)
    
    # Combine descriptions and voice transcripts
    content_parts <- sapply(1:nrow(cluster_videos), function(i) {
      video <- cluster_videos[i,]
      parts <- c()
      if (!is.na(video$video_description)) {
        parts <- c(parts, sprintf("Description: %s", video$video_description))
      }
      if (!is.na(video$voice_to_text)) {
        parts <- c(parts, sprintf("Voice transcript: %s", video$voice_to_text))
      }
      paste(parts, collapse = " | ")
    })
    
    paste(content_parts, collapse = "\n\n")
  }
  
  # Function to get label from OpenAI
  get_cluster_label <- function(content, retries = 3) {
    system_prompt <- "You are a researcher analyzing coordinated behavior on TikTok. 
Your task is to generate a concise (maximum 10 words), descriptive label in English that 
captures the main theme or characteristic of the content shared by a cluster of TikTok accounts.
Focus on the primary topic, political alignment (if evident), or content style that unifies these accounts.
The label should be factual and neutral in tone."
    
    user_prompt <- sprintf("Analyze the following content from a cluster of TikTok accounts and 
generate a concise label that describes their shared characteristics:

%s

Label:", content)
    
    for (attempt in 1:retries) {
      tryCatch({
        response <- httr::POST(
          url = "https://api.openai.com/v1/chat/completions",
          httr::add_headers(
            "Content-Type" = "application/json",
            "Authorization" = paste("Bearer", Sys.getenv("OPENAI_VERAAI_API_KEY")),
            "OpenAI-Organization" = Sys.getenv("OPENAI_VERAAI_ORG_ID")
          ),
          body = list(
            model = params$ai_model,
            messages = list(
              list(role = "system", content = system_prompt),
              list(role = "user", content = user_prompt)
            ),
            temperature = 0,
            max_tokens = 50
          ),
          encode = "json"
        )
        
        if (httr::status_code(response) == 200) {
          content <- httr::content(response)
          return(content$choices[[1]]$message$content)
        } else {
          log_warn(sprintf("Attempt %d failed with status code %d: %s", 
                         attempt, 
                         httr::status_code(response),
                         httr::content(response, "text")))
        }
      }, error = function(e) {
        log_warn(sprintf("Attempt %d failed: %s", attempt, conditionMessage(e)))
      })
      
      if (attempt < retries) Sys.sleep(2^attempt)
    }
    
    return("Failed to generate label")
  }
  
  # Get number of components
  num_components <- components(coordination_results$coord_graph)$no
  
  # Initialize progress bar
  pb <- progress_bar$new(
    format = "[:bar] :percent Labeling cluster :current/:total",
    total = num_components
  )
  
  # Process each component
  labels <- data.frame(
    component = integer(),
    label = character(),
    stringsAsFactors = FALSE
  )
  
  for (i in 1:num_components) {
    # Get content for this cluster
    content <- get_cluster_content(i)
    
    # Get label
    label <- get_cluster_label(content)
    
    # Add to results
    labels <- rbind(labels, data.frame(
      component = i,
      label = label,
      stringsAsFactors = FALSE
    ))
    
    # Update progress bar
    pb$tick()
    
    # Add small delay
    Sys.sleep(0.5)
  }
  
  # Save labels to file
  write_csv(labels, file.path(daily_output_dir, "cluster_labels.csv"))
  
  return(labels)
}

# Generate labels if we have coordination results
if (exists("coordination_results") && 
    !is.null(coordination_results$coord_graph) && 
    Sys.getenv("OPENAI_VERAAI_API_KEY") != "" &&
    Sys.getenv("OPENAI_VERAAI_ORG_ID") != "") {
    
  log_info("Starting cluster labeling")
  cluster_labels <- generate_cluster_labels(coordination_results, all_videos)
  
  # Add labels to clusters table if it exists
  if (!is.null(cluster_labels) && exists("clusters_table")) {
    clusters_table <- clusters_table %>%
      left_join(cluster_labels, by = c("Component" = "component")) %>%
      select(
        Component,
        Label = label,
        everything()
      )
  }
  
  log_info("Cluster labeling completed")
} else {
  log_warn("Skipping cluster labeling - missing coordination results or API credentials")
}

```

```{r save_output, include=FALSE, message=FALSE}

# Create necessary directories
tryCatch({
  dir.create(file.path(params$base_dir, "output"), showWarnings = FALSE, recursive = TRUE)
  dir.create(file.path(params$base_dir, "output", "coordination"), showWarnings = FALSE, recursive = TRUE)
}, error = function(e) {
  log_error(sprintf("Error creating directories: %s", conditionMessage(e)))
})

# Function to update account logs and save results
update_account_logs <- function(coordination_results) {
  today_date <- format(Sys.Date(), "%Y-%m-%d")
  
  # Define file paths
  daily_accounts_file <- file.path(daily_output_dir, "tiktok_coordinated_accounts_ids.csv")
  global_accounts_file <- file.path(params$base_dir, "lists", "tiktok_coordinated_accounts_ids.csv")
  log_file_path <- file.path(daily_output_dir, "new_accounts_log.csv")
  
  # Save daily snapshot
  if (exists("all_videos") && !is.null(all_videos)) {
    write_csv(all_videos, file.path(daily_output_dir, "daily_snapshot.csv"))
  }
  
  tryCatch({
    # Create output directory if it doesn't exist
    dir.create(dirname(log_file_path), showWarnings = FALSE, recursive = TRUE)
    
    # Read existing log file if it exists
    existing_log <- if (file.exists(log_file_path)) {
      read_csv(log_file_path, show_col_types = FALSE)
    } else {
      log_info("Creating new accounts log file")
      data.frame(
        Timestamp = as.Date(character()),
        New_Accounts_Count = numeric(),
        Total_Accounts = numeric()
      )
    }
    
    # Prepare the new log entry with correct column names
    new_entry <- data.frame(
      Timestamp = as.Date(Sys.Date()),
      New_Accounts_Count = nrow(coordination_results$new_accounts),
      Total_Accounts = length(coordination_results$updated_list)
    )
    
    # Append the new entry to the log file
    write_csv(
      bind_rows(existing_log, new_entry),
      log_file_path
    )
    
    log_info(sprintf("Added new log entry: %d new accounts, %d total accounts", 
                     new_entry$New_Accounts_Count, 
                     new_entry$Total_Accounts))
    
    # Update the coordinated accounts list
    if (length(coordination_results$updated_list) > 0) {
      # Save updated list in both the daily folder and the global folder
      write_csv(
        data.frame(x = coordination_results$updated_list),
        daily_accounts_file
      )
      
      write_csv(
        data.frame(x = coordination_results$updated_list),
        global_accounts_file
      )
      
      log_info(sprintf("Successfully updated coordinated accounts list with %d accounts in both locations", 
                       length(coordination_results$updated_list)))
    } else {
      log_warn("No accounts to update in the coordinated accounts list")
    }
    
    return(TRUE)
    
  }, error = function(e) {
    log_error(sprintf("Error updating account logs: %s", conditionMessage(e)))
    global_error_list <<- c(global_error_list, list(list(
      type = "update_logs_error",
      message = e$message
    )))
    return(FALSE)
  })
}


# Execute if we have coordination results
if (exists("coordination_results") && 
    !is.null(coordination_results$updated_list) && 
    !is.null(coordination_results$new_accounts)) {
  
  log_info("Starting to update account logs")
  update_success <- update_account_logs(coordination_results)
  
  if (update_success) {
    log_info("Successfully completed account log updates")
  } else {
    log_warn("Failed to update account logs")
  }
  
} else {
  log_warn("No coordination results available for logging")
}

# Save global error list if it contains any errors
if (length(global_error_list) > 0) {
  error_log_path <- file.path(daily_output_dir, "error_log.csv")
  
  # Create error data frame with proper error handling
  error_df <- tryCatch({
    do.call(rbind, lapply(global_error_list, function(x) {
      # Ensure all required fields exist with defaults
      data.frame(
        timestamp = as.Date(Sys.time()),
        type = if(!is.null(x$type)) x$type else "unknown",
        message = if(!is.null(x$message)) x$message else "no message",
        stringsAsFactors = FALSE
      )
    }))
  }, error = function(e) {
    # If there's an error creating the data frame, create an empty one with correct structure
    data.frame(
      timestamp = as.Date(character()),
      type = character(),
      message = character(),
      stringsAsFactors = FALSE
    )
  })
  
  # Only write if we have data
  if (nrow(error_df) > 0) {
    write_csv(error_df, error_log_path)
    log_info("Saved error log to file")
  } else {
    log_warn("No valid errors to log")
  }
}

```

```{r load_historical_data, include=TRUE, echo=FALSE}
# Function to load and combine historical data
load_historical_data <- function() {
  # Get all daily directories
  daily_dirs <- list.dirs(file.path(params$base_dir, "output", "daily"), recursive = FALSE)
  
  # Initialize empty lists for different types of data
  historical_data <- list(
    accounts = data.frame(),
    logs = data.frame(),
    summaries = list()
  )
  
  # Process each daily directory
  for (dir in daily_dirs) {
    date <- basename(dir)
    
    # Load account data
    account_file <- file.path(dir, "tiktok_coordinated_accounts_ids.csv")
    if (file.exists(account_file)) {
      accounts <- read_csv(account_file, show_col_types = FALSE) %>%
        mutate(date = as.Date(date))
      historical_data$accounts <- bind_rows(historical_data$accounts, accounts)
    }
    
    # Load log data
    log_file <- file.path(dir, "new_accounts_log.csv")
    if (file.exists(log_file)) {
      logs <- read_csv(log_file, show_col_types = FALSE)
      historical_data$logs <- bind_rows(historical_data$logs, logs)
    }
    
    # Load summary
    summary_file <- file.path(dir, "summary", "execution_summary.txt")
    if (file.exists(summary_file)) {
      historical_data$summaries[[date]] <- readLines(summary_file)
    }
  }
  
  return(historical_data)
}

# Load historical data
historical_data <- load_historical_data()
```

# Today Results

```{r results_summary, include=TRUE, echo=FALSE, results='asis'}

# Function to generate detailed summary
generate_summary <- function(
  coord_users,
  startdate,
  enddate,
  recent_videos,
  all_videos,
  coordination_results,
  global_error_list
) {
  # Initialize empty list for summary parts
  summary_parts <- list()
  
  # Basic collection summary
  summary_parts$collection <- sprintf(
    "We attempted to retrieve videos from **%s monitored accounts**, during the period from %s to %s.",
    scales::comma(length(coord_users)),
    format(as.Date(startdate), "%B %d, %Y"),
    format(as.Date(enddate), "%B %d, %Y")
  )
  
  # Recent videos summary
  if (is.data.frame(recent_videos) && nrow(recent_videos) > 0) {
    summary_parts$recent <- sprintf(
      "We successfully retrieved **%s recent videos**.",
      scales::comma(nrow(recent_videos))
    )
  } else {
    summary_parts$recent <- "We were unable to retrieve any recent videos due to API issues."
  }
  
  # All videos summary
  if (is.data.frame(all_videos) && nrow(all_videos) > 0) {
    summary_parts$all <- sprintf(
      "Using available data, we accessed a total of **%s videos** posted on TikTok within the timeframe.",
      scales::comma(nrow(all_videos))
    )
  } else {
    summary_parts$all <- "No videos were available for analysis."
  }
  
  # Coordination analysis summary
  if (!is.null(coordination_results) && 
      !is.null(coordination_results$summary_accounts) && 
      !is.null(coordination_results$coord_graph)) {
    
    summary_parts$coordination <- sprintf(
      "Our analysis for coordinated detection in these videos identified **%s accounts** spread across **%s components**, and it also uncovered **%s new accounts** exhibiting coordinated behavior.",
      scales::comma(nrow(coordination_results$summary_accounts)),
      scales::comma(igraph::components(coordination_results$coord_graph)$no),
      scales::comma(nrow(coordination_results$new_accounts))
    )
  } else {
    summary_parts$coordination <- "We were unable to perform coordinated detection analysis due to insufficient data."
  }
  
  # Generate error summary if there are errors
  error_summary <- generate_error_summary(global_error_list)
  
  # Combine all parts
  full_summary <- paste(unlist(summary_parts), collapse = " ")
  
  # Add error summary if there are errors
  if (nchar(error_summary) > 0) {
    full_summary <- paste(full_summary, "\n\n", error_summary)
  }
  
  # Create output directory if it doesn't exist
  dir.create(file.path(params$base_dir, "output", "summary"), showWarnings = FALSE, recursive = TRUE)
  
  # Save summary to file
  summary_file_path <- file.path(params$base_dir, "output", "summary", "execution_summary.txt")
  writeLines(c(full_summary), summary_file_path)
  
  # Log summary creation
  log_info("Generated execution summary")
  log_info(sprintf("Summary saved to %s", summary_file_path))
  
  return(list(
    summary = full_summary,
    error_summary = error_summary
  ))
}

# Function to generate error summary
generate_error_summary <- function(global_error_list) {
  # Only generate error summary if there are errors
  if (length(global_error_list) > 0) {
    # Create warning box with proper newlines and spacing
    warning_box <- sprintf(
      "::: {.callout-warning}\n\nWarning: %d errors were encountered during execution. Results may be incomplete. Please check the error summary below for details.\n\n:::\n\n",
      length(global_error_list)
    )
    
    # Rest of the code remains the same
    error_types <- table(sapply(global_error_list, function(x) x$type))
    
    error_parts <- c(
      '<details>',
      '<summary>Error Details</summary>',
      '\n',
      sprintf("**Total errors encountered:** %d\n", length(global_error_list)),
      "### Errors by type:\n",
      paste(capture.output(print(error_types)), collapse="\n"),
      "\n### Sample Errors:\n"
    )
    
    sample_size <- min(5, length(global_error_list))
    sample_errors <- sample(global_error_list, sample_size)
    error_samples <- sapply(sample_errors, function(error) {
      sprintf("- **Type:** %s, **Message:** %s", error$type, error$message)
    })
    
    error_details <- c(error_parts, error_samples, '</details>')
    
    return(paste(c(warning_box, error_details), collapse = "\n"))
  }
  
  # Return empty string if no errors
  return("")
}

# Execute summary generation
if (exists("coord_users") && exists("startdate") && exists("enddate")) {
  log_info("Generating execution summary")
  
  summary_result <- generate_summary(
    coord_users = coord_users,
    startdate = startdate,
    enddate = enddate,
    recent_videos = if(exists("recent_videos")) recent_videos else NULL,
    all_videos = if(exists("all_videos")) all_videos else NULL,
    coordination_results = if(exists("coordination_results")) coordination_results else NULL,
    global_error_list = global_error_list
  )
  
  # Print summary to console
  cat("\nExecution Summary:\n")
  cat(summary_result$summary)
  cat("\n")
} else {
  log_warn("Missing required variables for summary generation")
}

```

```{r historical_trends, include=TRUE, echo=FALSE, fig.cap="Historical trends of coordinated accounts"}
# Plot historical trends
ggplot(historical_data$logs, aes(x = Timestamp)) +
  geom_line(aes(y = Total_Accounts, color = "Total Accounts")) +
  geom_line(aes(y = New_Accounts_Count, color = "New Accounts")) +
  labs(
    title = "Historical Trends of Coordinated Accounts",
    x = "Date",
    y = "Number of Accounts",
    color = "Metric"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("Total Accounts" = "blue", "New Accounts" = "red"))
```

## Interactive Graph Visualization

```{r generate_network_function, include=FALSE, message=FALSE}

generate_network_visualization <- function(coordination_results,
                                        height = "800px",
                                        seed = 123,
                                        node_size_multiplier = 3,
                                        node_size_base = 10,
                                        edge_width_multiplier = 2) {
  
  if (!requireNamespace("htmltools", quietly = TRUE)) {
    stop("Package 'htmltools' is required")
  }
  
  tryCatch({
    if (is.null(coordination_results) || is.null(coordination_results$coord_graph)) {
      warning("No valid coordination graph available for visualization")
      return(NULL)
    }
    
    # Create output directory
    viz_dir <- file.path(daily_output_dir, "network")
    dir.create(viz_dir, showWarnings = FALSE, recursive = TRUE)
    
    # Get the graph
    graph <- coordination_results$coord_graph
    if (!igraph::is_igraph(graph)) {
      stop("Invalid graph object provided")
    }
    
    # Convert to visNetwork format first
    data <- visNetwork::toVisNetworkData(graph)
    
    # Basic node properties
    data$nodes$label <- data$nodes$id  # Ensure labels are set
    data$nodes$value <- igraph::degree(graph)
    data$nodes$title <- sprintf(
      "<div class='network-tooltip'>@%s<br>Connections: %d</div>",
      data$nodes$id,
      data$nodes$value
    )
    data$nodes$url <- paste0("https://tiktok.com/@", data$nodes$id)
    
    # Calculate component membership
    comp <- igraph::components(graph)$membership
    data$nodes$component <- comp
    
    # Add cluster labels if available
    if(exists("cluster_labels") && is.data.frame(get("cluster_labels")) && nrow(get("cluster_labels")) > 0) {
      # Create a lookup table for labels
      label_lookup <- get("cluster_labels") %>%
        dplyr::mutate(
          component_id = as.numeric(component),
          group_label = sprintf("Network %d - %s", component_id, label)
        ) %>%
        dplyr::select(component_id, group_label)
      
      # Join with nodes
      data$nodes <- data$nodes %>%
        dplyr::left_join(label_lookup, by = c("component" = "component_id")) %>%
        dplyr::mutate(
          size = value * node_size_multiplier + node_size_base,
          color = "#97C2FC",
          shape = "dot",
          shadow = TRUE,
          group = dplyr::coalesce(group_label, sprintf("Network %d", component))
        )
    } else {
      # If no labels available, use default numbering
      data$nodes <- data$nodes %>%
        dplyr::mutate(
          size = value * node_size_multiplier + node_size_base,
          color = "#97C2FC",
          shape = "dot",
          shadow = TRUE,
          group = sprintf("Network %d", component)
        )
    }
    
    # Add visual properties to edges
    data$edges <- data$edges %>%
      dplyr::mutate(
        width = if("weight" %in% names(.)) weight * edge_width_multiplier else 1,
        color = "#666666",
        smooth = FALSE
      )
    
    # Create minimalist CSS that won't interfere
    custom_css <- paste0(
      "<style>",
      "#network-viz-container { position: relative; }",
      "#network-viz-container .vis-network { position: relative; }",
      "#network-viz-container .network-tooltip { padding: 8px; background: white; border: 1px solid #ddd; border-radius: 4px; }",
      "#network-viz-container .vis-network-configuration { position: absolute; top: 10px; right: 10px; z-index: 100; background: white; padding: 8px; border-radius: 4px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }",
      "#network-viz-container .vis-network-configuration select { width: 200px; padding: 4px; border: 1px solid #ddd; border-radius: 4px; }",
      "</style>"
    )
    
    # Create the network visualization
    network <- visNetwork::visNetwork(
      nodes = data$nodes,
      edges = data$edges,
      width = "100%",
      height = height
    ) %>%
      visNetwork::visLayout(randomSeed = seed) %>%
      visNetwork::visOptions(
        highlightNearest = list(enabled = TRUE, degree = 1, hover = TRUE),
        selectedBy = list(
          variable = "group",
          main = "Select Network",
          style = "width: 200px; height: 30px; background-color: white;"
        ),
        nodesIdSelection = FALSE,
        manipulation = FALSE
      ) %>%
      visNetwork::visPhysics(
        stabilization = list(
          enabled = TRUE,
          iterations = 100
        )
      ) %>%
      visNetwork::visEvents(
        click = "function(params) {
          if (params.nodes.length > 0) {
            var nodeId = params.nodes[0];
            var nodeData = this.body.data.nodes.get(nodeId);
            if (nodeData.url) {
              window.open(nodeData.url, '_blank');
            }
          }
        }"
      ) %>%
      visNetwork::visInteraction(
        navigationButtons = TRUE,
        keyboard = TRUE
      )
    
    # Add container and CSS
    network <- htmlwidgets::onRender(
      network,
      sprintf(
        "function(el) {
          const container = document.createElement('div');
          container.id = 'network-viz-container';
          el.parentNode.insertBefore(container, el);
          container.appendChild(el);
          document.head.insertAdjacentHTML('beforeend', '%s');
        }",
        custom_css
      )
    )
    
    # Save visualization
    html_file <- file.path(viz_dir, "coordination_network.html")
    visNetwork::visSave(network, file = html_file, background = "white")
    
    message(sprintf(
      "Network visualization complete. Network has %d nodes and %d edges",
      igraph::gorder(graph),
      igraph::gsize(graph)
    ))
    
    return(network)
    
  }, error = function(e) {
    warning(sprintf("Error generating network visualization: %s", conditionMessage(e)))
    return(NULL)
  })
}

```

```{r generate_visualization, include=FALSE, message=FALSE}
# Generate the visualization and store it in the global environment
network_viz <- local({
  tryCatch({
    if (exists("coordination_results") && !is.null(coordination_results$coord_graph)) {
      generate_network_visualization(coordination_results)
    } else {
      warning("No coordination results available for network visualization")
      NULL
    }
  }, error = function(e) {
    warning(paste("Failed to generate network visualization:", e$message))
    NULL
  })
})

# Save the network_viz object to make it available across chunks
assign("network_viz", network_viz, envir = .GlobalEnv)
```

```{r display_visualization, include=TRUE, echo=FALSE}
#| label: fig-network
#| fig-cap: "Interactive visualization of coordinated accounts detected today. Click on a node to visit the respective TikTok account."

if (!is.null(network_viz)) {
  network_viz
} else {
  cat("No network visualization available for today's analysis.")
}
```

## Detailed Tables

```{r setup_tables, include=FALSE, eval=TRUE}

# Utility functions
create_url_link <- function(username, type = "profile") {
  if (type == "profile") {
    sprintf('<a href="https://tiktok.com/@%s" target="_blank">@%s</a>', username, username)
  } else if (type == "video") {
    sprintf('<a href="%s" target="_blank" class="btn btn-sm" style="background-color: #00926c; color: white; text-decoration: none; padding: 2px 8px; border-radius: 4px; font-size: 0.8em;">View Post</a>', username)
  } else {
    sprintf('<a href="%s" target="_blank">%s</a>', username, username)
  }
}

# Create base datatable with common options
create_datatable_base <- function(data, pageLength = 10, fontSize = "9px", 
                                orderCol = NULL, scrollX = TRUE) {
  options <- list(
    pageLength = pageLength,
    scrollX = scrollX,
    initComplete = JS(sprintf(
      "function(settings, json) { $(this.api().table().container()).css({'font-size': '%s'}); }",
      fontSize
    ))
  )
  
  if (!is.null(orderCol)) {
    options$order <- list(list(orderCol - 1, 'desc'))
  }
  
  datatable(data,
    options = options,
    escape = FALSE,
    rownames = FALSE
  )
}

# Table creation functions
create_posts_table <- function(summary_groups, all_videos, coord_graph) {
  # Get component membership for each account
  comp <- components(coord_graph)
  account_components <- data.frame(
    author_name = V(coord_graph)$name,
    network = comp$membership
  )
  
  # Create posts table
  posts_df <- summary_groups %>%
    left_join(
      select(all_videos, 
        video_description, video_url, author_name, create_time,
        view_count, like_count, comment_count, share_count
      ),
      by = c("object_id" = "video_description")
    ) %>%
    left_join(account_components, by = "author_name") %>%
    mutate(
      network = ifelse(!is.na(network), as.character(network), "NA"),
      video_link = map_chr(video_url, ~create_url_link(.x, "video")),
      Description = paste0(substr(object_id, 1, 150), "...")
    ) %>%
    select(
      `Post URL` = video_link,
      Network = network,
      Author = author_name,
      Timestamp = create_time,
      `Number of Accounts` = num_accounts,
      Views = view_count,
      Likes = like_count,
      Comments = comment_count,
      Shares = share_count,
      Description = object_id
    ) %>%
    distinct()

  create_datatable_base(posts_df, orderCol = 4)
}

create_accounts_table <- function(summary_accounts, coord_graph, all_videos) {
  # Get network metrics
  network_metrics <- list(
    degree = degree(coord_graph),
    strength = strength(coord_graph),
    betweenness = betweenness(coord_graph, normalized = TRUE)
  )
  
  metrics_df <- data.frame(
    account_id = names(network_metrics$degree),
    degree = as.numeric(network_metrics$degree),
    strength = as.numeric(network_metrics$strength),
    betweenness = as.numeric(network_metrics$betweenness)
  )
  
  # Get engagement metrics
  engagement_metrics <- all_videos %>%
    group_by(author_name) %>%
    summarize(
      avg_views = mean(view_count, na.rm = TRUE),
      avg_likes = mean(like_count, na.rm = TRUE),
      avg_comments = mean(comment_count, na.rm = TRUE),
      avg_shares = mean(share_count, na.rm = TRUE),
      .groups = 'drop'
    )
  
  # Create accounts table
  accounts_df <- summary_accounts %>%
    mutate(
      account_link = map_chr(account_id, ~create_url_link(.x, "profile")),
      network = components(coord_graph)$membership[match(account_id, V(coord_graph)$name)]
    ) %>%
    left_join(metrics_df, by = "account_id") %>%
    left_join(engagement_metrics, by = c("account_id" = "author_name")) %>%
    mutate(across(where(is.numeric), ~round(.x, 3))) %>%
    select(
      Account = account_link,
      Network = network,
      `Unique Shares` = unique_shares_count,
      `Avg Time Between Shares (sec)` = avg_time_delta,
      `Edge Symmetry Score` = avg_edge_symmetry_score,
      Degree = degree,
      Strength = strength,
      Betweenness = betweenness,
      `Avg Views` = avg_views,
      `Avg Likes` = avg_likes,
      `Avg Comments` = avg_comments,
      `Avg Shares` = avg_shares
    )
  
  create_datatable_base(accounts_df, pageLength = 25, orderCol = 6)
}

create_clusters_table <- function(coord_graph, summary_accounts, all_videos) {
  comp <- components(coord_graph)
  
  # Calculate cluster metrics
  cluster_metrics <- map_dfr(1:comp$no, function(i) {
    subg <- induced_subgraph(coord_graph, which(comp$membership == i))
    tibble(
      network = i,
      size = sum(comp$membership == i),
      density = edge_density(subg),
      diameter = if(vcount(subg) < 2) 0 else diameter(subg),
      transitivity = transitivity(subg, type = "global"),
      accounts = paste(V(coord_graph)$name[comp$membership == i], collapse = ", ")
    )
  })
  
  # Get engagement metrics by component
  engagement_metrics <- all_videos %>%
    mutate(
      network = comp$membership[match(author_name, V(coord_graph)$name)]
    ) %>%
    filter(!is.na(network)) %>%
    group_by(network) %>%
    summarize(
      avg_views = mean(view_count, na.rm = TRUE),
      avg_likes = mean(like_count, na.rm = TRUE),
      avg_comments = mean(comment_count, na.rm = TRUE),
      avg_shares = mean(share_count, na.rm = TRUE),
      .groups = 'drop'
    )
  
  # Create clusters table
  clusters_df <- cluster_metrics %>%
    left_join(engagement_metrics, by = "network") %>%
    mutate(across(where(is.numeric), ~round(.x, 3))) %>%
    select(
      Network = network,
      `Number of Accounts` = size,
      `Network Density` = density,
      `Network Diameter` = diameter,
      `Clustering Coefficient` = transitivity,
      `Avg Views` = avg_views,
      `Avg Likes` = avg_likes,
      `Avg Comments` = avg_comments,
      `Avg Shares` = avg_shares,
      `Member Accounts` = accounts
    )
  
  create_datatable_base(clusters_df, fontSize = "12px") %>%
    formatStyle(
      columns = 2:9,
      className = 'dt-right'
    ) %>%
    formatStyle(
      'Network',
      width = '300px'
    )
}

# Create all tables
if (exists("coordination_results") && exists("all_videos")) {
  accounts_table <- create_accounts_table(
    coordination_results$summary_accounts, 
    coordination_results$coord_graph,
    all_videos
  )
  
  clusters_table <- create_clusters_table(
    coordination_results$coord_graph, 
    coordination_results$summary_accounts,
    all_videos
  )
  
  posts_table <- create_posts_table(
    coordination_results$summary_groups, 
    all_videos,
    coordination_results$coord_graph
  )
}

```

```{r display_tables, echo=FALSE, results='asis'}
# Display tables with headers
cat("\n")  # Ensure proper spacing
```

### Coordinated Accounts
```{r display_accounts, echo=FALSE}
accounts_table
```

### Coordinated Networks/Clusters
```{r display_clusters, echo=FALSE}
clusters_table
```

### Coordinated Posts
```{r display_posts, echo=FALSE}
posts_table
```

```{r download-section, echo=FALSE}

# Function to prepare download files 
prepare_download_files <- function() {
 temp_dir <- tempdir()
 current_wd <- getwd()
 
 # Change to temp dir for flat zip structure
 setwd(temp_dir)
 
 # Save tables as CSV
 write.csv(accounts_table$x$data, "coordinated_accounts.csv", row.names = FALSE)
 write.csv(clusters_table$x$data, "network_clusters.csv", row.names = FALSE)
 write.csv(posts_table$x$data, "coordinated_posts.csv", row.names = FALSE)
 
 # Export network graph
 if (!is.null(coordination_results$coord_graph)) {
   igraph::write_graph(coordination_results$coord_graph, "coordination_network.graphml", format = "graphml")
 }
 
 # Create zip file with date
 current_date <- format(Sys.Date(), "%Y-%m-%d")
 daily_zip_name <- sprintf("coordination_results_%s.zip", current_date)
 zip_file <- file.path(daily_output_dir, daily_zip_name)
 files_to_zip <- list.files(pattern = "\\.(csv|graphml)$")
 if (length(files_to_zip) > 0) {
   utils::zip(zip_file, files = files_to_zip)
 }
 
 # Restore working directory
 setwd(current_wd)
 return(list(
   file = if (file.exists(zip_file)) zip_file else NULL,
   filename = daily_zip_name
 ))
}

# Create the zip file
zip_result <- prepare_download_files()

# Create download button HTML
if (!is.null(zip_result$file)) {
 download_url <- file.path("daily", format(Sys.Date(), "%Y-%m-%d"), zip_result$filename)
 download_html <- sprintf('
   <div style="background-color: #f8f9fa; padding: 20px; border-radius: 8px; margin: 20px 0;">
     <h3>Download Results</h3>
     <p>Download a comprehensive package of today\'s coordination analysis results, including:</p>
     <ul style="margin-bottom: 15px;">
       <li>Network visualization (HTML format)</li>
       <li>Coordinated accounts table (CSV format)</li>
       <li>Network clusters analysis (CSV format)</li>
       <li>Coordinated posts data (CSV format)</li>
       <li>Network graph (GraphML format)</li>
     </ul>
     <a href="%s" download class="btn btn-primary" style="display: inline-block; padding: 8px 16px; background-color: #0d6efd; color: white; text-decoration: none; border-radius: 4px; border: none;">
       Download Results Package
     </a>
   </div>
 ', download_url)
 
 htmltools::HTML(download_html)
}

```

# About

|                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                           |
|--------------------|----------------------------------------------------|
| [![vera ai logo](https://www.disinfo.eu/wp-content/uploads/elementor/thumbs/vera-logo_black-pz7er90kthmarde380cigj2nwx09ubmujp4y24avw2.jpg)](https://www.veraai.eu/) | [vera.ai](https://www.veraai.eu/home) is a research and development project focusing on disinformation analysis and AI supported verification tools and services. Project funded by EU Horizon Europe, the UK's innovation agency, and the Swiss State Secretariat for Education, Research and Innovation |

# References

Giglietto, F., Marino, G., Mincigrucci, R., & Stanziano, A. (2023). A Workflow to Detect, Monitor, and Update Lists of Coordinated Social Media Accounts Across Time: The Case of the 2022 Italian Election. Social Media + Society, 9(3). https://doi.org/10.1177/20563051231196866

# Session Info

```{r session_info, include=TRUE, echo=FALSE}
sessionInfo()
```
